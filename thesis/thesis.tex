\documentclass[titlepage]{report}
\usepackage[ngerman]{babel}
\usepackage[backend=biber,style=numeric]{biblatex}
\addbibresource{literature.bib}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{hyphenat}
\usepackage{glossaries}
\usepackage{array}
\usepackage{calc}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{bytefield}
\usepackage{float}
\usepackage{eurosym}
\usepackage{tabu}
\usepackage{caption}
\lstset{%
    frame=tb,
    tabsize=4,
    numbers=left,
    breaklines=true,
}
\setcounter{biburllcpenalty}{9001}
\renewcommand{\lstlistingname}{Quelltext}
\renewcommand{\lstlistlistingname}{Quelltextverzeichnis}
\makeglossaries{}
\input{glossaries.tex}
\title{Evaluierung eines Messpunkte-Clusters für Netzwerktests auf dem
Campus der TU Clausthal}
\author{Christian Rebischke\\
\gls{tuc}\\
Rechenzentrum\\
Matrikelnummer: 432108 \\
Studiengang: Informatik (Bachelor) \\
E-Mail: Christian.Rebischke@tu-clausthal.de}
\begin{document}
\maketitle
\chapter*{Danksagung}
Ich bedanke mich bei dem Rechenzentrum der \gls{tuc}, insbesondere bei
Dipl.\hyp{}Math. Christian Strauf. Das Thema dieser Bachelorarbeit
beruht auf seiner Idee und war mein Ansporn, mich mit diesem Thema näher
auseinanderzusetzen. Des Weiteren danke ich Herrn Prof. Dr.\hyp{}Ing. Dr.
rer. nat. habil. Harald Richter für die Unterstützung aus akademischer
Seite. Besonderen Dank bekommt von mir auch die
Open Source Gemeinschaft.  Ohne die harte Arbeit der
Open Source Gemeinschaft würden die grundlegenden Werkzeuge, die
ich zur Vollendung dieser Bachelorarbeit benutzt habe, nicht
existieren. Besonders hilfreich waren das Softwarepaket \LaTeX{} und die
Zeichensoftware \emph{Draw IO} (\url{https://www.draw.io/}).
\chapter*{Eidesstattliche Erklärung}
Hiermit erkläre ich an Eides statt, dass ich die vorliegende Arbeit
selbstständig und nur unter Zuhilfenahme der ausgewiesenen Hilfsmittel
angefertigt habe. Sämtliche Stellen der Arbeit, die im Wortlaut oder dem
Sinn nach anderen gedruckten oder im Internet verfügbaren Werken
entnommen sind, habe ich durch genaue Quellenangaben kenntlich gemacht.
Außerdem wurde diese Arbeit in gleicher oder ähnlicher Form noch keiner
anderen Prüfungsstelle im Sinne von \S 11 Absatz 5 lit. b) der
allgemeinen Prüfungsordnung vorgelegt.
\\
\\
Clausthal-Zellerfeld, der \today
\\
\\
Christian Rebischke
\chapter*{Sperrvermerk}
Hiermit erkläre ich mich damit einverstanden, dass meine Bachelorarbeit
in der Instituts- und/oder der Universitätsbibliothek ausgelegt
und zur Einsichtnahme aufbewahrt werden darf.
\\
\\
Clausthal-Zellerfeld, der \today
\\
\\
Christian Rebischke
\tableofcontents
\chapter{Vorwort}
Es gibt nun das Internet seit circa 45 Jahren und das \gls{www} seit 24
Jahren und keine Technologie
ist über so kurze Zeit so alltäglich geworden. Das Internet hat es
geschafft, Einzug zu erhalten in Arbeit, Privatleben und auch Forschung
und Lehre. Nahezu in allen wissenschaftlichen Disziplinen spielt das
Internet und die damit verknüpfte Informationstechnologie eine Rolle.
Sei es die Industrie 4.0 mit ihren cyber\hyp{}physischen Systemen, dem
schnellen Abgleich von DNA-Informationen über das Netz in der
angewandten Biologie, dem Sammeln von Krankheitsdaten in der Medizin
oder das Verarbeiten von Datenmengen gigantischen Ausmaßes im
Finanzsektor. All diese Beispiele sind nur möglich durch immer größere
Technologiesprünge in der Informatik und dem immer größeren Ausbau des
Internets. Da ist es nicht verwunderlich, dass der UN-Menschenrechtsrat das
Internet zu einem Menschenrecht\cite{UNHRC} erklärt hat und umso weniger
verwunderlich ist es, dass die Vernetzung von Computersystemen auch auf
dem Campus der \gls{tuc} eine Rolle spielt,
nicht nur für Forschung und Lehre, sondern auch für den
täglichen Betrieb. Eine Schlüsselposition nimmt dabei das Rechenzentrum
der \gls{tuc} ein. Das Rechenzentrum bildet die
Basis für die Vernetzung der einzelnen Fakultäten untereinander, der
Vernetzung zwischen Fakultäten und Firmen aus der freien Wirtschaft,
sowie auch die Vernetzung zwischen der \gls{tuc}
und anderen Universitäten weltweit. Dementsprechend wichtig ist ein
stabiles Netz für den täglichen Betrieb. In dieser Bachelorarbeit widme
ich mich deshalb der technischen Umsetzung eines verteilten
Monitoring\hyp{}Systems zur Überwachung der Netzwerkqualität zwischen
einzelnen Endpunkten und Kernsystemen, die für einen problemlosen
Netzbetrieb nötig sind. Das Rechenzentrum der \gls{tuc} dient bei dieser
Bachelorarbeit als Auftraggeber.
\chapter{Problemstellung}
Das Netz der \gls{tuc} erstreckt sich über mehrere
Standorte. Teilweise liegen diese Standorte nicht in Clausthal
selbst, wie beispielsweise das \gls{efzn} in Goslar. Dementsprechend
schwierig gestaltet sich die Wartung und der Betrieb
des Netzes. So kann auf Netzeinbrüche etwa nur reaktiv
nach Meldung des Problems reagiert werden. Es existiert
zwar ein Monitoring\hyp{}System, welches die Verfügbarkeit von
einzelnen Diensten überprüft, jedoch erfolgt diese Messung
von einem Punkt aus und gibt binäre Statuswerte
zurück (Dienst läuft oder Dienst läuft nicht). Dementsprechend
fehlen Informationen um die Verfügbarkeit von Diensten und
deren vollständige Funktion von mehreren Messpunkten aus
zu garantieren. Beispielsweise ist es möglich, dass ein Dienst
zwar vom zentralen Monitoring\hyp{}Server aus erreichbar ist, aber
aus einem einzelnen Institut der Zugriff auf den Dienst nur
eingeschränkt oder sogar gar nicht möglich ist. Das Rechenzentrum der
\gls{tuc} bietet mehrere Kerndienste an. Dazu
gehören:
\begin{itemize}
    \item \gls{dns}
    \item Diverse Webdienste basierend auf:
    \begin{itemize}
        \item \gls{http}
        \item \gls{https}
    \end{itemize}
    \item \gls{cifs}
    \item \gls{voip}
    \item \gls{smtp}
\end{itemize}

\autoref{fig:problemstellung} zeigt die aktuelle Netzstruktur der \gls{tuc}. Zu sehen
sind die drei \emph{Core\hyp{}Router}, welche auf die Gebiete
Rechenzentrum, Feldgraben und Tannenhöhe verteilt sind.
\emph{Core\hyp{}Router} stellen das Rückgrat, den Backbone, des
\gls{lan}
der \gls{tuc} dar und leiten den Datenverkehr zwischen den einzelnen am
Netz angeschlossenen Geräten. Sie trennen außerdem das Netz in logische
Abschnitte. Zu sehen ist ebenfalls das aktuell existierende
Monitoring\hyp{}System, welches einzelne Kerndienste überwacht. Diese
Kerndienste und das aktuelle Monitoring\hyp{}System sind im Rechenzentrum
beheimatet. Dadurch entsteht eine physikalische und auch logische Nähe
der Systeme. Durch diese Nähe verlässt der Datenverkehr, welcher die
Kerndienste überprüft, niemals die LANs des Rechenzentrum. Dies führt dazu, dass
einzelne Kerndienste zwar als in Betrieb und fehlerfrei angezeigt
werden, aber durchaus die Möglichkeit besteht, dass einzelne Dienste
nicht von jedem Rechner der \gls{tuc} der \gls{tuc}
Zustand ermittelt wird, ist unklar, wie groß die Latenz zwischen den
Diensten und den jeweiligen Endkunden ist. Diese Latenz ist allerdings
entscheidend und hat Einfluss auf die Nutzererfahrung der Endkunden.
Besonders Dienste wie \gls{dns} sind auf Verbindungen mit einer geringen
Latenz (der Zeit zwischen dem Absenden des ersten Bytes eines Pakets und
dessen Empfang) angewiesen. Eine zu hohe Latenz zwischen einem Client und dem Dienst
führt unweigerlich zu für den Nutzer spürbaren Konsequenzen (zum
Beispiel zu verzögerten Seitenaufrufe beim Web-Browsing). Noch mehr ins
Gewicht fallen Latenzen bei \gls{voip}, dort sind Latenzen oder ein
Jitter (die Varianz der Laufzeit der Datenpakete\cite{JITTERWIKI})
leicht auszumachen. Außerdem ist anzumerken, dass bei \gls{voip} eine
Mindestbandbreite vorhanden ist, da sonst der Codec (Deutsch:
Dekodierer) schlecht arbeitet. Was fehlt, ist ein System aus verteilten
Messpunkten in den \glspl{lan} der \gls{tuc},
das es ermöglicht, die Erreichbarkeit einzelner Dienste periodisch und
über einen längeren Zeitraum zu beobachten (siehe \autoref{fig:problemloesung}). Dies
hätte mehrere Vorteile: Zum einen ließe sich so der Zustand des
Campus-Netzwerks besser erfassen, da Tests nicht nur von einem zentralen
Monitoring\hyp{}System aus gestartet werden. Zum anderen können die
gewonnenen Daten weiter verwertet, grafisch aufbereitet und zum Beispiel
für die Erstellung von Langzeitstatistiken über die Gesundheit des
Netzwerks genutzt werden. Der Zustand des Netzwerkes wird somit als
Funktion der Zeit angesehen. Weiterhin könnten im Fall eines Ausfalls die
zuständigen Netzadministratoren benachrichtigt werden, im Idealfall
durch gewohnte Kommunikationswege wie E\hyp{}mail, aber auch durch neue
Kommunikationswege wie in etwa das Absetzen von Tweets oder
Chatnachrichten (beispielsweise an den Chat\hyp{}Server der \gls{tuc}:
\url{https://chat.rz.tu-clausthal.de}). Außerdem wäre es durch
einen dezentralen Aufbau einfacher, das System beliebig zu skalieren
und auf Wachstum und Schrumpfen des Netzes zu reagieren. Mit dem Wandel
von einem zentralen zu einem dezentralen Monitoring\hyp{}System entsteht
allerdings auch mehr Arbeitsaufwand, denn auch diese Systeme müssen
gewartet werden. Dies umfasst das Verwalten von Konfigurationsdateien,
das Aktualisieren und Installieren von Software und die Installation des
Grundsystems. Als zentrale Forschungsfrage ergibt sich somit: \emph{Wie
lassen sich Rechnernetze dezentral und skalierbar überwachen?}
Insgesamt lassen sich daraus folgende Herausforderungen an diese Arbeit
ableiten:

\begin{itemize}
    \item Es müssen Server im Campusnetz gefunden werden, welche sich
          als verteilter Messpunkt eignen oder diese durch den Einsatz
          von neuer Hardware im Campusnetz platziert werden.
    \item Es muss eine Software\hyp{}Plattform gebaut oder eine
          vorhandene Software\hyp{}Plattform erweitert werden, sodass
          sie den Anforderungen bezüglich Monitoring aus der
          Problemstellung gerecht wird.
    \item Es muss ein Modell für die gespeicherten Daten gefunden
          werden.
    \item Es muss ein Weg zur Datenvisualisierung gefunden werden.
    \item Das System muss horizontal skalierbar sein. Das heißt, es
          muss um Messpunkte erweiterbar sein.
    \item Das System muss bei wachsendem Komplexitätsgrad und Anzahl von
          Messpunkten steuerbar und kontrollierbar bleiben.
      \item Das System muss über \gls{tcp}/\gls{ip} oder
           \gls{udp}/\gls{ip} basierte Rechnernetze
           kommunizieren. Dies schließt die Kommunikation über logische
           Teilnetze wie \glspl{vlan} oder \glspl{vxlan} mit ein.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/problemstellung.pdf}
    \caption{Veranschaulichung der Problemstellung anhand der aktuellen
    Netzstruktur der TU Clausthal ohne die Anbindungen Goslar und Celle
    über das öffentliche Internet.}\label{fig:problemstellung}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/problemloesung.pdf}
    \caption{Veranschaulichung der Problemlösung anhand der aktuellen
    Netzstruktur der TU Clausthal ohne die Anbindungen Goslar und Celle
    über das öffentliche Internet.}\label{fig:problemloesung}
\end{figure}
\chapter{Technische Grundlagen}
In diesem Kapitel werden die für die Lösung des Problems nötigen
technischen Grundlagen erläutert. Außerdem werden die Fehlerfälle für
die einzelnen Protokolle definiert. Unter technische Grundlagen fallen
einige wichtige Netzwerkprotokolle, sowie eine allgemeine Einführung in
Computernetzwerke.
\section{Netzwerkgrundlagen}
Um auf Basis der vorangegangenen Problemstellung eine Lösung zu
erarbeiten, ist es notwendig, einen groben Überblick über die Grundlagen
von Computernetzwerken zu bekommen. Als Basis dafür dient das \gls{osi}. Das
\gls{osi} ist de facto das bis heute gängige Referenzmodell, wenn es
darum geht, mehrere Systeme miteinander zu vernetzen. Ein solches System
wird \emph{offenes System} genannt, wenn es den im \gls{osi}
spezifizierten Standards entspricht\cite[Siehe Abschnitt
4.1.2]{ITUOSI}. Diese \emph{offenen Systemen} sind mit einem physischen
Medium verbunden und bilden so ein Computernetzwerk. \autoref{fig:opensystems}
zeigt eine solche Verbindung mehrerer \emph{offener Systeme}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/open_systems.pdf}
    \caption{Stark vereinfachte Darstellung der Assoziationen verschiedenener offener
    Systeme mit diversen Anwendungen über ein physisches Medium.}\label{fig:opensystems}
\end{figure}
Innerhalb eines dieser \emph{offener Systeme} sind sieben
Netzwerkschichten oder auch \emph{OSI-Schichten} definiert\cite[Siehe
Abschnitt 6.1.2]{ITUOSI}. Zur
Kommunikation zwischen zwei \emph{offenen Systemen} werden mindestens
die beiden untersten Schichten durchlaufen, die Bitübertragungsschicht
für den Transport der Bits und die Sicherungsschicht für den Zugriff auf
den Bittransport.
\autoref{fig:osi} listet alle sieben Schichten, deren Protokolle,
Einheiten und Einordnung auf. Die Funktionsweise des \gls{osi}s lässt
sich am besten durch ein kurzes Beispiel erklären:

Es wird angenommen, dass von einem Laptop eine Website aufgerufen wird.
Nachfolgend wird nur der Netzverkehr zwischen dem Laptop und dem
Webserver
betrachtet. Wenn der Client eine \gls{http}\hyp{}basierte Website
aufrufen möchte, sendet dieser eine \gls{http}\hyp{}Anfrage in Form von
eines
\gls{http}\hyp{}Requests, der den Beginn des \gls{http}\hyp{}Protokolls
darstellt (Anwendungsschicht). Diese \gls{http}\hyp{}Daten
werden wiederum in über \gls{tcp} in 64 Kilobyte große
\gls{tcp}\hyp{}Pakete verpackt (Transportschicht),
welches wiederum über 64 Kilobyte große Pakete des
\gls{ip}\hyp{}Protokolls dem Ziel zugestellt werden
(Vermittlungsschicht). Diese \gls{ip}\hyp{}Pakete enthalten die Adresse
des Empfängers und werden in viele Ethernet-Datenrahmen mit einer Länge
von 1500 Bytes eingebettet (Sicherungsschicht), welche in Form von
kodierten Bits über ein Netzwerkkabel übertragen werden (Bitübertragungsschicht).
Dieses Matroschka\hyp{}ähnliche Gebilde wird über das physische
Medium bitseriell versendet und der Empfänger packt es, angefangen bei der
Bitübertragungsschicht, aufsteigend wieder aus.

\textbf{Anmerkung}: Es handelt sich hierbei um eine stark vereinfachte
Darstellung. In der Realität kommen diverse andere Faktoren dazu, wie in
etwa:
\begin{itemize}
    \item Das Auflösen eines Hostnamen mit \gls{dns}
    \item Das Routing über \gls{ip}
    \item Der Aufbau einer Session mit \gls{tcp}
    \item Der ungesicherte Transport mit \gls{udp}
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/osi.pdf}
    \caption{Das OSI-Schichtenmodell mit Protokollbeispielen und
    verwendeten Einheiten}\label{fig:osi}
\end{figure}
\section{\glsfirst{dns}}
Aus Erfahrungen mit dem Internetvorgänger \gls{arpanet} wurde
abgeleitet, dass ein manuelles Eingeben von \gls{ip}\hyp{}Adressen mit
steigender Anzahl von Knoten im Netzwerk immer unübersichtlicher wurde.
Hinzukommend sind \gls{ip}\hyp{}Adressen für den Menschen schwer zu
merken. Mit dieser Problemstellung als Grundlage arbeitete der Ingenieur
Peter Mockapetris an einem ersten Lösungsansatz: \glsfirst{dns}.
Bei \gls{dns} handelt es sich um einen mehr als 30 Jahre alten
Verzeichnisdienst, welcher über das gleichnamige Protokoll für Menschen
merkbare symbolische Namen auf \gls{ip}\hyp{}Adressen abbildet.
\gls{dns} wurde erstmalig im Jahr 1983 in den beiden \glspl{rfc}
\gls{rfc} 882\cite{RFC0882} und \gls{rfc} 883\cite{RFC0883} beschrieben.
Damals befanden sich die \gls{dns}\hyp{}Einträge, die Abbildungen von
lesbarer Adresse auf \gls{ip}\hyp{}Adresse, noch verteilt auf
allen Servern des frühen Internets und wurden vom \gls{nic} verwaltet
und mit dem Dateiübertragungsprotokoll \gls{ftp}
synchronisiert\cite{RFC1034}. In der damaligen Zeit stellte sich dies
als ein Flaschenhals für das Internet heraus, da die Anzahl der Server
im Netzwerk exponentiell zunahm. Deshalb wurden nur vier Jahre später
Überarbeitungen von \gls{dns} veröffentlicht. Diese Überarbeitungen
wurden in den \glspl{rfc} 1034 und 1035 erläutert
und bilden die Grundlage für \gls{dns} wie es heute bekannt ist und auch
eingesetzt wird. Der heutige Ansatz verläuft dezentraler, als es damals
der Fall gewesen ist. Anstatt die \gls{dns}\hyp{}Einträge auf allen
Knoten des Internets zu verteilen und zentral vom \gls{nic} aus zu
steuern, existieren heute mehrere hierarchische Verwaltungsebenen. Dazu
wird der \gls{fqdn} hierarchisch gegliedert. Ein Beispiel für einen
\gls{fqdn} ist \emph{akira.rz.tu-clausthal.de}. Die
\autoref{fig:dnsname} veranschaulicht die Gliederung dieses \gls{fqdn} in die
einzelnen Bestandteile.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/dnsname.pdf}
    \caption{Bestandteile eines \gls{fqdn} mit optionalem Hostname und Third-Level
    Domain}\label{fig:dnsname}
\end{figure}
Eine Angabe des Hostname oder einer Third-Level
Domain ist optional. Es können auch weitere Domains hinzugefügt werden.
So ist zum Beispiel auch eine 6th-Level Domain möglich. Die maximale Anzahl der
Subdomains ist in keinem der \gls{dns} \glspl{rfc} spezifiziert. Deshalb
ist die maximale Anzahl abhängig vom \gls{dns}\hyp{}Server, der die
Domains ausliefert. Nach \gls{rfc} 1035 ist die maximale Länge eines
\gls{fqdn} aber auf 255 Bytes begrenzt\cite[siehe Section
2.3.4]{RFC1035}, was die maximale Anzahl von Subdomains zumindest stark
einschränkt. Der Begriff Subdomain umfasst alle Domains unter der
\gls{tld}. \glspl{fqdn} unterliegen einer Baumstruktur. Dessen Wurzel ist die
\emph{Root Domain}, meistens symbolisiert durch einen einfachen Punkt.
Eine Ebene direkt darunter sind die \glspl{tld}. Diese
werden in der Regel von den \glspl{nic} einzelner Länder verwaltet. Von
Ländern verwaltete \glspl{tld} haben meist Länderabkürzungen wie
\emph{de} (Deutschland), \emph{jp} (Japan) oder \emph{cn} (Volksrepublik
China). Es existieren aber auch militärische oder akademische
\glspl{tld} wie \emph{edu} und \emph{mil}. In Deutschland ist die DENIC eG
verantwortlich für Domains mit \emph{de}\hyp{}Endung.
In den letzten 5 Jahren kamen auch noch neue \glspl{tld} hinzu, welche
nicht an Länder geknüpft sind. Darunter fallen Markennamen wie
\emph{BMW}, \emph{Audi} oder \emph{Deutschepost} oder markenfreie Namen
wie \emph{academy}, \emph{fun} oder \emph{house}\cite{NEWTLDLIST}. Die
Vergabe der \glspl{tld} erfolgte direkt über die \gls{icann}. Der jeweilige
Käufer ist Nutzer der jeweiligen \gls{tld}, solange er die jährlichen
Gebühren dafür bezahlt. Ein Nutzer einer \gls{tld} kann beliebig viele Subdomains erstellen für den
Eigenbedarf, diese weiterverkaufen oder gar verschenken. Weitere
wichtige Elemente des \gls{dns}\hyp{}Protokolls sind \emph{Name Server}
und \emph{Resolver}\cite[siehe Section 2.4]{RFC1034}.
\emph{Name Server} sind Server\hyp{}Programme, welche
Informationen zur Namensauflösung enthalten. Ergo halten \emph{Name
Server} Domain\hyp{}\gls{ip}\hyp{}Tabellen für ihre
hierarchische Ebene vor und cachen sonstige Abbildungen, wie in etwa die
Abbildung von \gls{dns}\hyp{}Name auf \gls{ip}\hyp{}Adressen, die durch sie
hindurch geschleust werden. Wenn kein Eintrag zu dem angefragten \gls{fqdn}
existiert, wird der nächsthöhere \gls{dns}\hyp{}Server gefragt. An
oberster Stelle steht ein \emph{Root Nameserver
Cluster}\cite{ROOTNAMESERVER}. Die \emph{Root
Name Server} kennen die Namen und \gls{ip}\hyp{}Adressen aller
\emph{Name Server}, die für die \gls{tld} verantwortlich sind.
\emph{Resolver} übernehmen die Rolle des Clients, sie senden
\gls{dns}\hyp{}Anfragen an \emph{Name Server}. Diese
Anfragen erfolgen in den meisten Fällen über das \gls{udp} an den Zielport
53 des Name\hyp{}Servers\cite[Siehe Section 4.2.1]{RFC1035}. Es ist allerdings auch \gls{dns}
über \gls{tcp} spezifiziert\cite[Siehe Section 4.2.2]{RFC1035}.
Letzteres ist nötig für die \gls{dns}\hyp{}Erweiterungen
\gls{dns}\hyp{}over\hyp{}\gls{tls} (Port 853) und
\gls{dns}\hyp{}over\hyp{}\gls{https} (Port 443), welche das Protokoll
\gls{https} für
\gls{dns} nutzt und somit Verschlüsselung und Authentifizierung bereitstellt \cite{RFC7858}
(\gls{dns}\hyp{}over\hyp{}\gls{https} ist zum Zeitpunkt dieser Arbeit
noch nicht endgültig spezifiziert). Dieses Bestreben, das
\gls{dns}\hyp{}Protokoll sicherer
zu machen, unterstreicht dessen Wichtigkeit für das Internet. Ohne
\gls{dns} wären diverse vernetzte Anwendungen nicht möglich. Jeder
Webbrowser oder Mailclient beispielsweise erzeugt diverse
Namensauflösungen durch den lokalen Resolver. Ist nur
eine dieser Anfragen fehlerhaft oder stark verzögert ist das für den
Nutzer augenblicklich zu merken. Im Fall von fehlerhaften Anfragen wäre
eine Auflösung der Domain nicht möglich und eine Verbindung zu der
assoziierten \gls{ip}\hyp{}Adresse würde fehlschlagen. Der Webbrowser
würde in diesem Fall einen Fehlerbildschirm anzeigen.
Bei stark verzögerten
\gls{dns}\hyp{}Auflösungen dauert der Aufbau einer frisch angeforderten
Webseite länger als üblich. Wie wichtig schnelle Antwortzeiten eines Computers
sind, hat Walter Doherty bereits im Jahre 1982 mit seinem Paper \emph{The
Economic Value of Rapid Response Time} deutlich gemacht. Doherty
beobachtete einen signifikanten Anstieg an Benutzerinteraktionen, wenn
die Latenz der Anfrage (in diesem Fall bezogen auf normale
Computereingaben) unter einen bestimmten Wert fiel\cite{DOHERTY}.
Bezogen auf die Latenz von \gls{dns}\hyp{}Anfragen hieße das, dass der
Benutzer bei zu hoher Latenz massiv bei seiner Arbeit ausgebremst wird. Dies führt nicht nur
zu Frustration beim Nutzer, sondern auch zu Verschwendung seiner
bezahlten Arbeitszeit und damit zu einer geringeren Produktivität.
\section{\glsfirst{http}}
\glsfirst{http} ist ein auf \gls{tcp}/\gls{ip} aufbauendes
Datenübertragungsprotokoll, welches in der Anwendungsschicht des
\gls{osi}s operiert. Der am meisten verbreiteste Anwendungszweck für das
Protokoll ist das Ausliefern von Webseiten über Port 80. \gls{http}
wurde erstmals im Jahre 1996 als \gls{rfc} 1945 spezifiziert, nachdem 
es bereits fast sechs Jahre im Internet im Einsatz gewesen
war\cite{RFC1945}. Im Laufe der Jahre kamen weitere Versionen,
sowie diverse Erweiterungen für das Protokoll hinzu. Darunter
Erweiterungen für Kompression (um die Datenübertragungsrate zu erhöhen),
Verschlüsselung, Caching und Authentifikation. Seit 2015 ist die
aktuelle Version \gls{http}/2, welche in \gls{rfc} 7540 standardisiert
werden\cite{RFC7540}. \gls{http} ist ein zustandsloses Protokoll,
dementsprechend werden mehrere Anfragen getrennt voneinander und ohne
Kontext zueinander bearbeitet. Es wird keine Session aufgebaut und
keine Sitzungsinformationen verwaltet. Persistente Sitzungsinformationen 
werden durch \emph{Cookies} übertragen. \emph{Cookies} sind im
\gls{http}\hyp{}Header des \gls{http}\hyp{}Response befindliche
Schlüssel\hyp{}Werte\hyp{}Paare,
welche der Sitzung eine eindeutige \emph{Session ID}
zuweisen\cite{RFC2965}\cite{RFC6265}\cite{RFC2109}.
Zur Interaktion mit dem Server besitzt \gls{http} mehrere Methoden. Die häufigsten sind:
\begin{description}
    \item[GET] um an mit der \gls{url} verknüpfte Informationen zu
        kommen. Die Antwort auf einen \textbf{GET}\hyp{}Aufruf
        beinhaltet den Header und den Body des
        \gls{http}\hyp{}Response\cite[Siehe Section 9.3]{RFC2616}.
    \item[POST] wird benutzt, um Informationen an den Server zu senden.
        Diese Informationen werden im \gls{http}\hyp{}Header als
        Schlüssel\hyp{}Wert\hyp{}Paare übermittelt\cite[Siehe Section
        9.5]{RFC2616}. Dies eignet sich besonders für kritische Informationen, wie zum Beispiel
        Passwörter. Da der \gls{http}\hyp{}Header bei \gls{https}
        verschlüsselt wird. Die eigentliche Funktion des
        \textbf{POST}\hyp{}Aufrufs ist dem Server überlassen\cite[Siehe
        Section 9.3]{RFC2616}
    \item[HEAD] ähnlich wie \textbf{GET} mit dem Unterschied, dass
        der Server bei einer Antwort keinen Inhalt mitliefert. Es wird
        nur der \gls{http}\hyp{}Header übertragen\cite[Siehe Section
        9.4]{RFC2616}, welcher meistens
        einen Titel und eine Kurzbeschreibung der Webseite umfasst.
\end{description}
Die Methoden \textbf{DELETE}, \textbf{PUT} sind nur für eine
\gls{restapi} relevant und umfassen das Löschen und hinzufügen. Um
herauszufinden, welche Methoden ein Server unterstützt, kann die Methode
\textbf{OPTIONS} verwendet werden.
Im \autoref{lst:httpanfrage} ist eine \gls{http}\hyp{}Anfrage mit Antwort und
Verbindungsaufbau an den Host \url{http://tu-clausthal.de}
dargestellt. Die eigentliche \gls{http}\hyp{}Anfrage beginnt ab Zeile 5
und endet in Zeile 9. Aus Zeile 5 wird ersichtlich, dass es sich um die
Version 1.1 von \gls{http} handelt und der Client via \emph{GET} den
Index der Seite \emph{tu-clausthal.de} (siehe Zeile 6) anfragt. Zeile 7
übermittelt den Namen und Version der Software mit, der diese
\gls{http}\hyp{}Anfrage gesendet worden ist und Zeile 8 definiert,
welche Mediatypen in einer Antwort erlaubt sind\cite{RFC2616}. Ab Zeile
10 beginnt der \gls{http}\hyp{}Response des  Servers. Dort werden Protokoll und die
Version nochmal bestätigt und in diesem Beispiel ein Statuscode
hinzugefügt (301 weist darauf hin, dass der Inhalt der Seite verschoben
worden ist und an einem neuen Ort liegt (siehe Zeile 13)). Dazu wird auf
eine neue \gls{url} verwiesen. Die \gls{url} dient als Pfadangabe zur
gewünschten Web\hyp{}Ressource oder Datei. Bei \gls{http} haben diese
Pfadangaben den Präfix \emph{http://}. Zeile 11 gibt
das aktuelle Datum, die Uhrzeit und die Zeitzone an und Zeile 12 die
Software des Servers und deren Version. Die Zeilen 14 bis 16
enthalten die Information, dass der Server diverse Encodings unterstützt
(zum Beispiel Kompression mit dem Kompressionsalgorithmus \emph{gzip}),
die Länge des übermittelten Inhalts und der Typ des Inhalts sowie dessen
Zeichenkodierung. Die Zeilen 18 bis 21 enthalten den übermittelten
Inhalt, hier gekürzt dargestellt.
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Eine HTTP-Anfrage an
http://tu-clausthal.de},label={lst:httpanfrage}]
* Rebuilt URL to: http://tu-clausthal.de/
*   Trying 2001:638:605:20:1::2...
* TCP_NODELAY set
* Connected to tu-clausthal.de (2001:638:605:20:1::2) port 80 (#0)
> GET / HTTP/1.1
> Host: tu-clausthal.de
> User-Agent: curl/7.59.0
> Accept: */*
>
< HTTP/1.1 301 Moved Permanently
< Date: Fri, 11 May 2018 23:48:50 GMT
< Server: Apache/2.2.22 (Ubuntu)
< Location: http://www.tu-clausthal.de/
< Vary: Accept-Encoding
< Content-Length: 235
< Content-Type: text/html; charset=iso-8859-1
<
<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
<html><head>
[...]
</body></html>
* Connection #0 to host tu-clausthal.de left intact
\end{lstlisting}
\end{minipage}
Bei der Verwendung von \gls{http} können unter anderem folgende Fehlerfälle auftreten:
\begin{itemize}
    \item Eine zu geringe Bandbreite oder eine zu hohe Latenz führen zu
        erheblichen Einschränkungen beim Seitenaufbau und damit in einem
        für den Nutzer zu langsamen Tempo. Im schlimmsten Fall kommt es
        zu einem Timeout und es wird der \gls{http}\hyp{}Statuscode 408
        zurückgegeben\cite[Siehe Section 10.4.9]{RFC2068}.
    \item Der Webserver ist nicht in der Lage die Anfrage zu
        verarbeiten. Dieser Fehler terminiert mit dem Statuscode
        503\cite[Siehe Section 10.5.4]{RFC2068}.
    \item Die mit der angegebenen \gls{url} verknüpfte
        Web\hyp{}Ressource konnte nicht gefunden werden. Als Statuscode
        wird 404 verwendet\cite[Siehe Section 10.4.5]{RFC2068}.

Weitere Fehler und Statuscodes können dem \gls{rfc} 2068 entnommen werden\cite[Siehe Section 10]{RFC2068}.
\end{itemize}
\section{\glsfirst{https}}
Bei \gls{https} handelt es sich um das Protokoll \gls{http} mit einer
zusätzlichen Schicht zur Verschlüsselung und Herstellung von
Datenintegrität. Dafür wird das Verschlüsselungsprotokoll \gls{tls}
benutzt, teilweise auch noch unter der Bezeichnung \gls{ssl} bekannt.
Dabei ist anzumerken, dass \gls{ssl} das Vorgängerprotokoll von
\gls{tls} ist.
Für den Verbindungsaufbau wird bei \gls{https} standardmäßig
Port 433 benutzt\cite[Siehe Section 2.3]{RFC2818}. Als
\gls{url}\hyp{}Präfix dient \emph{https://}. Bei der Verschlüsselung und
Herstellung der Datenintegrität bleibt die eigentliche
\gls{http}\hyp{}Syntax wie sie ist, ergo handelt es sich um eine Verschlüsselung der
einzelnen \gls{http}\hyp{}Pakete für Request und Response. Dazu verschickt der Client beim
Verbindungsaufbau über Port 433 ein \emph{TLS ClientHello} an den
Server, woraufhin ein \gls{tls}\hyp{}Handshake initiiert wird.
Dieser Handshake beinhaltet die Überprüfung der Integrität des WebServers
unter Betrachtung des \gls{tls}\hyp{}Zertifikats. Das Zertifikat ist
ein öffentlicher Schlüssel, welcher mit einer
digitalen Signatur einer Zertifizierungsstelle beglaubigt worden ist.
Der Client besitzt eine Datenbank mit gültigen Zertifizierungsstellen
und vergleicht so das signierte Zertifikat des Servers mit allen
gespeicherten Zertifizierungsstellen. Wurden keine Mängel
festgestellt, beispielsweise ein abgelaufenes Zertifikat oder eine
fremde Zertifizierungsstelle, geht der Client davon aus, dass der Server
die Identität besitzt, die er vorgibt zu haben. Dies ist insofern
problematisch, als in der Vergangenheit wiederholt bei
Zertifizierungsstellen eingebrochen worden ist und private Schlüssel zum
Signieren von gültigen Zertifikaten gestohlen worden
sind\cite{PRIVATEKEYSSTOLEN}. Nach der Integritätsprüfung folgt der
eigentliche Aufbau einer verschlüsselten Verbindung. Für den Aufbau
existieren derzeit zwei mögliche Verfahren. Zum einen der
\gls{rsa}\hyp{}Handshake und zum anderen der
Diffie\hyp{}Hellman\hyp{}Handshake\cite{TLS}. Beim
\gls{rsa}\hyp{}Handshake wird vom Client ein symmetrischer Schlüssel
erzeugt, dieser wird mit dem öffentlichen Schlüssel des Servers
verschlüsselt und das Ergebnis an den Server übermittelt. Der Server entschlüsselt den
symmetrischen Schlüssel unter Zuhilfenahme seines privaten Schlüssels.
Damit ist eine sichere Verbindung aufgebaut und der Client und der Server
sind in der Lage, sich mithilfe des symmetrischen Schlüssels
verschlüsselte Nachrichten zu senden. Beim
Diffie\hyp{}Hellman\hyp{}Handshake dagegen werden die öffentlichen
Schlüssel beider Gesprächspartner ausgetauscht und mit dem jeweiligen im
Besitz befindlichen privaten Schlüssel ein gemeinsamer symmetrischer
Schlüssel berechnet. Dieser symmetrische Schlüssel verlässt im Gegensatz
zum \gls{rsa}\hyp{}Handshake niemals den Server oder Client. Außerdem
ist es möglich, für jede Session einen neuen flüchtigen symmetrischen Key
zu erzeugen. Dies wird ermöglicht, in dem bei jeder neuen Session ein
neues Schlüsselpaar erzeugt wird. Deshalb ist der
Diffie\hyp{}Hellman\hyp{}Handshake als sicherer anzusehen, da der
gemeinsame symmetrische Schlüssel niemals übertragen wird (auch nicht
verschlüsselt) und neue Sessions immer mit einem neuen Schlüssel
versehen werden. Letzteres ermöglicht \gls{pfs}. Durch \gls{pfs} ist es
einem Angreifer nicht möglich, ältere aufgezeichnete Verbindungen zu
entschlüsseln, auch wenn er in den Besitz eines privaten Schlüssels
gelangt.
Die Fehlerfälle bei \gls{https} entsprechen denen von \gls{http}, 
erweitert um einige Fehler bezogen auf \gls{tls}:
\begin{itemize}
    \item Ein \gls{tls}\hyp{}Zertifikat ist ungültig
    \item Es kommt keine Verbindung auf Port 443 zustande, weil der
          Webserver kein \gls{https} unterstützt.
\end{itemize}
\section{\glsfirst{cifs}}
\gls{cifs} ist ein von der Firma Microsoft 1996 eingeführtes
Datentransferprotokoll auf Basis von \gls{smb}\cite{SMBWIKI} und
\gls{netbios} über \gls{tcp}/\gls{ip}. Das Protokoll ist nicht nur auf
beschränkt, sondern kann auch für Druckerfreigaben,
Windows-RPC (ein von Microsoft eingeführtes Protokoll, um Code aus der
Ferne auszuführen) und den NT-Domänendienst (ein von Microsoft
eingeführter Dienst zur Authentifizierung von Computern und Nutzern)
verwendet werden. Für die in dem vorherigen Kapitel genannte
Problemstellung ist allerdings nur der Dateitransfer via \gls{smb} von
Relevanz. Im Gegensatz zu \gls{http} ist \gls{cifs} ein
sessionbehaftetes Protokoll. Der \gls{cifs}\hyp{}Server ordnet also
jeder Verbindung eine Sitzung zu, die einem Client genau zugeordnet
werden kann. Darüber sind diverse Operationen möglich wie
Authentifizierung, Verschlüsselung oder \emph{Locking}\cite[S.
16]{MSSMB}. Beim \emph{Locking} wird der Zugriff auf einen Nutzer
beschränkt, um deren Beschädigung durch mehrfaches Schreiben an
derselben Stelle  zu vermeiden. Um dies zu verhindern,
setzt der \gls{cifs}\hyp{}Server ein \emph{Lock} auf diese Datei und
lässt nur einen Client in diese Datei schreiben. Der eigentliche
Transfer der Dateien wird mit \gls{tcp} vor Datenübertragungsfehlern geschützt.
Die von der \gls{iana} für \gls{cifs} vergebene Portnummer ist:
445\cite[S. 19]{MSSMB}. Des Weiteren hat \gls{tcp} den Vorteil, dass es
\emph{full-duplex} ist. \emph{Full-duplex} bedeutet, dass \gls{tcp} in
der Lage ist, gleichzeitig Daten zu empfangen und zu senden.
\autoref{fig:smbbytefield} zeigt den Aufbau eines solchen \gls{smb}\hyp{}Pakets mit
\gls{tcp}\hyp{}Header. Der Header beginnt mit einem Byte aus Nullen und
der anschließenden Länge des \gls{smb}\hyp{}Pakets. Danach folgt in
32\hyp{}Byte\hyp{}Blöcken die eigentliche Nachricht. Die Länge des Pakets wird als
drei Byte Integer in \emph{Network Byte Order} repräsentiert\cite[S.
21]{MSSMB}. \emph{Network Byte Order} entspricht dem \emph{Big Endian
Format}, bei dem das höchstwertige Byte zuerst gespeichert wird.
\begin{figure}[h]
    \centering
    \begin{bytefield}{32}
        \bitheader{0-31}\\
        \bitbox{8}{Null-Byte} & \bitbox{24}{Länge des SMB-Pakets}\\
        \bitbox{32}{SMB Nachricht}\\
        \bitbox{32}{\ldots}
    \end{bytefield}
    \caption{Aufbau eines \gls{smb}\hyp{}Pakets in der TCP-Nutzlast}\label{fig:smbbytefield}
\end{figure}
Bei der Verwendung von \gls{cifs} können folgende Fehlerfälle auftreten:
\begin{itemize}
    \item Die Verbindung zum Server, welcher den \gls{cifs}\hyp{}Share
          anbietet, weist eine hohe Latenz auf. Dies führt zu langsamen
          Schreib\hyp{} und Leseoperationen auf dem Share.
    \item Der \gls{cifs}\hyp{}Share ist nicht erreichbar.
\end{itemize}
\chapter{Herleitung eines Lösungsansatzes}
\section{Anforderungsanalyse}
Nachfolgend werden die ermittelten funktionalen und nicht\hyp{}funktionalen
Anforderungen erläutert. Funktionale Anforderungen stellen das
``eigentliche Systemverhalten und die jeweiligen Funktionen des zu
erstellenden Produkts''\cite[S. 20]{BPSE} dar, also die grundlegenden
Aufgaben der Software im Bezug auf die Problemstellung. Die
nichthyp{}funktionalen Anforderungen dagegen sind besonders. Sie umfassen
Anforderungen wie Sicherheit, nachträgliche Erweiterbarkeit,
Testbarkeit, also Anforderungen, welche erst nach der Entwicklung
mess\hyp{} oder testbar werden\cite[S. 292]{SNFA}. Um die funktionalen
und nichthyp{}funktionalen Anforderungen besser einordnen zu können, werden
folgende Schlüsselwörter zum Kennzeichnen für Anforderungen nach
\gls{rfc} 2119\cite{RFC2119} definiert:
\begin{description}
    \item[MUSS] ist eine absolute Anforderung an die Software. Alle
        Anforderungen, die mit \textbf{MUSS} markiert sind,
        \textbf{MÜSSEN} implementiert werden.
    \item[DARF NICHT] ist gleichbedeutend zu \textbf{VERBOTEN}.
    \item[SOLL] ist eine Anforderung, die implementiert werden
        \textbf{SOLLTE}, aber nicht \textbf{MUSS}. Dies ist der Fall bei
        Anforderungen, welche aus nachvollziehbaren Gründen nicht
        implementiert werden.
    \item[SOLL NICHT] ist gleichbedeutend zu \textbf{NICHT EMPFOHLEN}
        und beschreibt Anforderungen, die nicht erfüllt werden sollten,
        wenn sie vermieden werden können.
    \item[KANN] ist eine Anforderung die implementiert werden
        \textbf{KANN}. Diese Art von Anforderungen sind
        zusätzliches Extra und nicht nötig für die Grundfunktion der
        Software.
\end{description}
\textbf{Anmerkung}: Das \gls{rfc} 2119 ist im Original in Englisch. Ich
habe mich zur übersetzung der Schlüsselwörter auf die Übersetzung der
Schweizer Firma Adfinis SyGroup AG gestützt\cite{RFC2119DE}.
\subsection{Funktionale Anforderungen}
\begin{center}
\begin{tabular}{p{0.7\textwidth-\tabcolsep}>{\raggedleft\arraybackslash}p{0.3\textwidth-\tabcolsep}}\toprule
    \textbf{FA1: Überwachung von \gls{dns} } & \textbf{Priorität: MUSS} \\\midrule
	\multicolumn{2}{p{\textwidth-\tabcolsep}}{%
        Die Software \textbf{muss} die Erreichbarkeit mehrerer
        \gls{dns}\hyp{}Server überprüfen können. Ebenfalls \textbf{muss} die
        Latenz einer \gls{dns}\hyp{}Anfrage gemessen werden können.}\\\bottomrule
\end{tabular}
    \captionof{table}{Funktionale Anforderung FA1}\label{table:FA1}
\end{center}
\begin{center}
\begin{tabular}{p{0.7\textwidth-\tabcolsep}>{\raggedleft\arraybackslash}p{0.3\textwidth-\tabcolsep}}\toprule
    \textbf{FA2: Überwachung von \gls{http} } & \textbf{Priorität: MUSS} \\\midrule
	\multicolumn{2}{p{\textwidth-\tabcolsep}}{%
        Die Software \textbf{MUSS} die Erreichbarkeit mehrerer
        \gls{http}\hyp{}Server überprüfen können. Ebenfalls
        \textbf{muss} die Latenz einer \gls{http}\hyp{}Anfrage gemessen werden können.}\\\bottomrule
\end{tabular}
    \captionof{table}{Funktionale Anforderung FA2}\label{table:FA2}
\end{center}
\begin{center}
\begin{tabular}{p{0.7\textwidth-\tabcolsep}>{\raggedleft\arraybackslash}p{0.3\textwidth-\tabcolsep}}\toprule
    \textbf{FA3: Überwachung von \gls{https} } & \textbf{Priorität: MUSS} \\\midrule
	\multicolumn{2}{p{\textwidth-\tabcolsep}}{%
        Die Software \textbf{MUSS} die Erreichbarkeit mehrerer
        \gls{https}\hyp{}Server überprüfen können. Ebenfalls \textbf{muss} die
        Latenz einer \gls{https}\hyp{}Anfrage gemessen werden können.}\\\bottomrule
\end{tabular}
    \captionof{table}{Funktionale Anforderung FA3}\label{table:FA3}
\end{center}
\begin{center}
\begin{tabular}{p{0.7\textwidth-\tabcolsep}>{\raggedleft\arraybackslash}p{0.3\textwidth-\tabcolsep}}\toprule
    \textbf{FA4: Überwachung von \gls{cifs} } & \textbf{Priorität: SOLL} \\\midrule
	\multicolumn{2}{p{\textwidth-\tabcolsep}}{%
    Die Software \textbf{SOLL} die Zeit messen können, die
    zwischen einer \gls{cifs}-Abfrage und der Antwort von einem
    \gls{cifs}\hyp{}Server vergeht.}\\\bottomrule
\end{tabular}
    \captionof{table}{Funktionale Anforderung FA4}\label{table:FA4}
\end{center}
\begin{center}
\begin{tabular}{p{0.7\textwidth-\tabcolsep}>{\raggedleft\arraybackslash}p{0.3\textwidth-\tabcolsep}}\toprule
    \textbf{FA5: Speicherung von Performance-Daten in einer Datenbank } & \textbf{Priorität: MUSS} \\\midrule
	\multicolumn{2}{p{\textwidth-\tabcolsep}}{%
        Das System \textbf{MUSS} die gesammelten Performance-Daten
        zur weiteren Auswertung an eine Datenbank übertragen.}\\\bottomrule
\end{tabular}
    \captionof{table}{Funktionale Anforderung FA5}\label{table:FA5}
\end{center}
\begin{center}
\begin{tabular}{p{0.7\textwidth-\tabcolsep}>{\raggedleft\arraybackslash}p{0.3\textwidth-\tabcolsep}}\toprule
    \textbf{FA6: Grafische Aufbereitung } & \textbf{Priorität: MUSS} \\\midrule
	\multicolumn{2}{p{\textwidth-\tabcolsep}}{%
        Die vom System zur Datenbank gesendeten Performance-Daten
        \textbf{MÜSSEN} für die Administratoren grafisch
        aufbereitet werden.
        Diese Graphen \textbf{MÜSSEN} via Port 80 (\gls{http})
        und Port 443 (\gls{https}) erreichbar sein.
        }\\\bottomrule
\end{tabular}
    \captionof{table}{Funktionale Anforderung FA6}\label{table:FA6}
\end{center}
\begin{center}
\begin{tabular}{p{0.7\textwidth-\tabcolsep}>{\raggedleft\arraybackslash}p{0.3\textwidth-\tabcolsep}}\toprule
    \textbf{FA7: Bandbreitenmessung } & \textbf{Priorität: MUSS} \\\midrule
	\multicolumn{2}{p{\textwidth-\tabcolsep}}{%
        Das System \textbf{MUSS} in der Lage sein Bandbreitenmessungen anhand des
        Durchsatzes vorzunehmen.
                }\\\bottomrule
\end{tabular}
    \captionof{table}{Funktionale Anforderung FA7}\label{table:FA7}
\end{center}
\begin{center}
\begin{tabular}{p{0.7\textwidth-\tabcolsep}>{\raggedleft\arraybackslash}p{0.3\textwidth-\tabcolsep}}\toprule
    \textbf{FA8: Native 1 Gigabit Ethernet Schnittstelle} & \textbf{Priorität: SOLL} \\\midrule
	\multicolumn{2}{p{\textwidth-\tabcolsep}}{%
    Die Hardware der Messpunkte \textbf{SOLL} über eine native 1 Gigabit
    Ethernet Schnittstelle verfügen.
    }\\\bottomrule
\end{tabular}
    \captionof{table}{Funktionale Anforderung FA8}\label{table:FA8}
\end{center}
\begin{center}
\begin{tabular}{p{0.7\textwidth-\tabcolsep}>{\raggedleft\arraybackslash}p{0.3\textwidth-\tabcolsep}}\toprule
    \textbf{FA9: Kontrollierbarkeit} & \textbf{Priorität: MUSS} \\\midrule
	\multicolumn{2}{p{\textwidth-\tabcolsep}}{%
        Das zu entwickelnde System \textbf{MUSS} bei wachsendem
        Komplexitätsgrad und Anzahl von Messpunkten steuerbar und
        kontrollierbar bleiben. Dies betrifft auch die Komponenten
        Datenbank und Visualisierungslösung.
        }\\\bottomrule
\end{tabular}
    \captionof{table}{Funktionale Anforderung FA9}\label{table:FA9}
\end{center}

\subsection{Nicht-funktionale Anforderungen}
\begin{center}
\begin{tabular}{p{0.7\textwidth-\tabcolsep}>{\raggedleft\arraybackslash}p{0.3\textwidth-\tabcolsep}}\toprule
    \textbf{NFA1: Wahl der Programmiersprache} & \textbf{Priorität: SOLL} \\\midrule
	\multicolumn{2}{p{\textwidth-\tabcolsep}}{%
        Das System \textbf{SOLL} in einer dem Rechenzentrum der
        \gls{tuc} gängigen Programmiersprache entwickelt werden.
        Folgende Programmiersprachen werden im Rechenzentrum der
        \gls{tuc} täglich benutzt:
        \begin{itemize}
            \item Python
            \item Bash
            \item PHP
            \item Javascript
        \end{itemize}
    }\\\bottomrule
\end{tabular}
    \captionof{table}{Nicht-Funktionale Anforderung NFA1}\label{table:NFA1}
\end{center}
\begin{center}
\begin{tabular}{p{0.7\textwidth-\tabcolsep}>{\raggedleft\arraybackslash}p{0.3\textwidth-\tabcolsep}}\toprule
    \textbf{NFA2: Niedrige Beschaffungskosten} & \textbf{Priorität: SOLL} \\\midrule
	\multicolumn{2}{p{\textwidth-\tabcolsep}}{%
    Die Hardware der Messpunkte \textbf{SOLL} möglichst günstig in der Beschaffung
    sein.
    }\\\bottomrule
\end{tabular}
    \captionof{table}{Nicht-Funktionale Anforderung NFA2}\label{table:NFA2}
\end{center}
\begin{center}
\begin{tabular}{p{0.7\textwidth-\tabcolsep}>{\raggedleft\arraybackslash}p{0.3\textwidth-\tabcolsep}}\toprule
    \textbf{NFA3: Sicherheit} & \textbf{Priorität: SOLL} \\\midrule
	\multicolumn{2}{p{\textwidth-\tabcolsep}}{%
        Das System \textbf{SOLL} sicher konzipiert sein.
        Alle Übertragungen, welche sensible Daten übertragen,  \textbf{SOLLEN} mit gängigen
        als sicher eingestuften Algorithmen verschlüsselt sein.
        }\\\bottomrule
\end{tabular}
    \captionof{table}{Nicht-Funktionale Anforderung NFA3}\label{table:NFA3}
\end{center}
\begin{center}
\begin{tabular}{p{0.7\textwidth-\tabcolsep}>{\raggedleft\arraybackslash}p{0.3\textwidth-\tabcolsep}}\toprule
    \textbf{NFA4: Horizontale Skalierbarkeit} & \textbf{Priorität: MUSS} \\\midrule
	\multicolumn{2}{p{\textwidth-\tabcolsep}}{%
        Das zu entwickelnde System \textbf{MUSS} horizontal skalierbar
        sein.
        }\\\bottomrule
\end{tabular}
    \captionof{table}{Nicht-Funktionale Anforderung NFA4}\label{table:NFA4}
\end{center}

\section{Systemarchitektur}
Auf Grundlage der Problemstellung und der Anforderungen ergibt sich die
folgende Aufstellung von Komponenten für die Systemarchitektur:
\begin{description}
    \item[Datenbank] Eine Datenbank zur Speicherung der
                     gewonnenen Messdaten.
    \item[Konfigurationsmanagement] Ein Subsystem zur Verwaltung, Kontrolle
        und Konfiguration der einzelnen Messsensoren, der Datenbank und
        der Visualisierungslösung.
    \item[Visualisierungslösung] Das Subsystem zur Visualisierung der, von
        den Messsensoren, gewonnenen Messdaten.
    \item[Messsensor] Ein verteilter Messpunkt, um die in den
        Anforderungen spezifizierten Daten zu gewinnen.
\end{description}
Aus der Komponentenaufstellung ergeben sich die Assoziationen zwischen
den einzelnen Komponenten. Das Konfigurationsmanagement umspannt alle
Komponenten und sorgt für deren Konfiguration, Verwaltung und Kontrolle.
Dadurch ist es möglich die \gls{mttr} so kurz wie möglich zu halten. Die
\gls{mttr} bezeichnet die mittlere Zeit die benötigt wird um ein System
im Fehlerfall wiederherzustellen\cite{MTTR}. Nur durch den Einsatz von
Konfigurationsmanagement auf allen Systemkomponenten ist es möglich
diese, im Falle eines Systemausfalls, innerhalb
wenigen Minuten automatisiert aus den Konfigurationsdateien wieder
herzustellen. Außerdem ergibt sich durch die Verwendung eines
Konfigurationsmanagement mit Versionsverwaltung eine Historie der
Veränderungen an dieser Komponente. Die Konfiguration ist
nachvollziehbar und identisch reproduzierbar.
Die einzelnen Messensoren senden ihre Messdaten zur Datenbank. Der
Webserver mit der Visualisierungslösung stellt diese Daten für den Nutzer aufbereitet
 grafisch dar.
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/solution.pdf}
    \caption{Erster stark vereinfachter Entwurf einer möglichen Lösung}\label{fig:solution}
\end{figure}
\section{Datenbanken}
Zur Speicherung der Messdaten steht ein breites Spektrum von Datenbanken
zur Verfügung. Das Spektrum reicht von traditionellen, relationalen
Datenbanken, wie zum Beispiel MySQL, bis zu modernen
NoSQL\hyp{}Datenbanken, wie Key\hyp{}Value\hyp{}Stores und \glspl{tsdb}.
Dieses Kapitel versucht einen Überblick über alle Datenbanktypen zu
geben und den passenden Datenbanktyp für das Projekt festzulegen.
\subsection{Relationale Datenbanken}
Bei relationalen Datenbanken handelt es sich um tabellenbasierte Datensätze.
Als Grundlage dieser Datenbanken dienen Relationen. Relationen sind
algebraische Strukturen in Form von n\hyp{}Tupel. Also in sich selbst
abgeschlossene Tautologien. Für Schreib\hyp{} und Leseoperationen auf
diesen durch Relationen beschrieben Tabellen wird eine
formale Sprache verwendet: die relationale Algebra. Jede Tabelle wird beim
Relationenmodell als eine Menge untergeordneter Tupel aufgefasst\cite[S.
4]{MEIER2013}. Ein weiteres Merkmal von relationalen Datenbanken ist die
Verwendung von Primärschlüsseln (Englisch: primary key). Durch Primärschlüsseln
ist jeder Eintrag in einer relationalen Datenbank eindeutig identifizierbar. Um
Datenbankeinträge abzufragen, zu verändern und in die Datenbank zu
transferieren, wird die international standardisierte
Datenbankbeschreibungssprache \gls{sql} eingesetzt. Die nachfolgende Tabelle
namens \emph{Studenten} beschreibt einen Datenbankeintrag in einer relationen
Datenbank:
\begin{center}
    \begin{tabular}{l c r}
        \toprule
        Matrikelnummer & Vorname & Nachname \\
        \midrule
        1              & Alice   & Alcatraz \\
        2              & Bob     & Bounty   \\
        3              & Charlie & Echo     \\
        \bottomrule
    \end{tabular}
\end{center}
In der oben abgebildeten Datenbank ist nur eine Tabelle namens
\emph{Studenten} vorhanden. Auf dieser Tabelle können mit \gls{sql} diverse
Operationen durchgeführt werden. \autoref{lst:sqlexample} zeigt das Hinzufügen neuer
Daten, das Abrufen von Daten, das Manipulieren von Daten und das
Erzeugen einer neuen Tabelle. Als Implementierung für das Beispiel
\autoref{lst:sqlexample} wurde \emph{SQLite}
verwendet. \emph{SQLite} ist eine quelloffene und äußerst
leichtgewichtige Datenbank\cite{SQLITE}. In der ersten Zeile wird durch das \emph{INSERT} Statement
ein neuer Student zur Tabelle \emph{Studenten} mit der Matrikelnummer 4,
dem Vornamen Dana und dem Nachnamen Foxtrott hinzugefügt. In der zweiten
Zeile werden alle Vornamen aus der Tabelle \emph{Studenten}
ausgegeben und in der siebten Zeile wird der Nachname vom Studenten
namens Charlie zu Alcatraz geändert. Zeile 8 zeigt das Erzeugen einer
neuen Tabelle \emph{Professoren} mit der MitarbeiterID als
aufsteigenden Primärschlüssel und den Einträgen zu Vorname und Nachname.

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Verwendung von Structured Query Language},label={lst:sqlexample}]
sqlite> INSERT INTO Studenten VALUES (4,'Dana','Foxtrott');
sqlite> select Vorname from Studenten;
Alice
Bob
Charlie
Dana
sqlite> UPDATE Studenten SET Nachname = "Alcatraz" WHERE Matrikelnummer = 3;
sqlite> CREATE TABLE Professoren(MitarbeiterID INTEGER PRIMARY KEY ASC, Vorname, Nachname);
\end{lstlisting}
\end{minipage}
Bekannte Implementierungen für relationale Datenbanken sind:
\begin{itemize}
    \item MySQL
    \item MariaDB
    \item SQLite
    \item Microsoft SQL Server
\end{itemize}
\subsection{NoSQL\hyp{}Datenbanken}
NoSQL\hyp{}Datenbanken sind zunächst ein grober Sammelbegriff für alle
Datenbanken die kein relationales Konzept verfolgen und somit nicht
durch die Sprache \gls{sql} beschrieben werden können. In den meisten
Fällen handelt es sich bei NoSQL\hyp{}Datenbanken um einfache
Key\hyp{}Value\hyp{}Stores. Key\hyp{}Value\hyp{}Stores sind Ansammlungen
von Daten in n\hyp{}Tupeln mit einem Schlüssel und einem oder mehrerer
Werte. Sie ähneln
demnach, also einem Wörterbuch. Meist sind diese Ansammlungen von Tupeln
in großen Binärdateien (\gls{blob}) oder sogar in einfachen Textdateien
gespeichert. Ein Beispiel für eine Key\hyp{}Value\hyp{}Datenbank ist
\emph{Dynamo} von der Firma \emph{Amazon}. Doch NoSQL\hyp{}Datenbanken
umfassen nicht nur Key\hyp{}Value\hyp{}Stores. Es existieren auch
NoSQL\hyp{}Datenbanken zum Speichern von Graphen, großen Mengen an
Dokumenten, wie \emph{IBM Notes}, Spaltenorientierten Datenbanken wie
\emph{Apache Cassandra} und der \glsfirst{tsdb}. \glspl{tsdb} kommen bei
jedem Gebiet zum Einsatz, wo große Mengen zeit veränderlicher Daten
gespeichert werden müssen. \glspl{tsdb} sammeln große Datenmengen und
benutzen einen Zeitstempel als Index für diese Daten. Jedes Datum wird
demnach einem festen Zeitpunkt zugeordnet. Dies macht eine \gls{tsdb}
besonders beliebt als Datenbank für Wetterdaten,
Aktienhandelsdaten, Messdaten oder sonstige Daten die abhängig von der
Zeit und in besonders großen Mengen gespeichert werden sollen. Beispiele
für \glspl{tsdb} sind \emph{InfluxDB} und die \gls{tsdb} im
Monitoring\hyp{}System \emph{Prometheus}. Trotz dieser Kategorisierungen
haben NoSQL\hyp{}Datenbanken mehrere gemeinsame Eigenschaften.
Beispielsweise verzichten NoSQL\hyp{}Datenbanken auf Eigenschaften von
relationalen Datenbanken, um die Skalierbarkeit zu erhöhen\cite[S. 13]{TSDB}. Dadurch sind
NoSQL\hyp{}Datenbanken simpler aufgebaut und haben weniger Ansprüche an
die gespeicherten Daten. Es müssen keine Datentypen eingehalten oder
sonstige Relationen zwischen Daten gewährleistet werden, insbesondere
gibt es auch keine ACID\hyp{}Eigenschaften. Die Kurzform ACID 
bedeutet\cite{NOSQLKRAMER}\cite{SQLLEIPZIG}:
\begin{description}
    \item[Atomarität] Entweder komplette Ausführung der Transaktion oder
        keine Ausführung.
    \item[Konsistenzerhaltung] Wenn die Datenbank vor der Transaktion in
        sich in einem konsistenten Zustand befand, soll sich die
        Datenbank nach der Transaktion immer noch in einem konsistenten
        Zustand befinden.
    \item[Isolation] Transaktionen sind voneinander isoliert. Dies
        bedeutet, dass laufende Transaktionen sich nicht gegenseitig
        beeinflussen können.
    \item[Dauerhaftigkeit] Abgeschlossene Transaktionen sind garantiert
        dauerhaft in der Datenbank gespeichert.
\end{description}
Stattdessen kommt in NoSQL\hyp{}Datenbanken das BASE\hyp{}Paradigma zum
Einsatz. BASE ist die Kurzform für \emph{basically available, soft
state, eventually consistent} (Deutsch: Grundsätzlich verfügbar, weicher
Zustand, eventuell konsistent). Im Gegensatz zu pessimistischen
\gls{sql}\hyp{}Datenbanken verfolgen NoSQL\hyp{}Datenbanken ein
optimistisches Paradigma\cite[Siehe Seite 51]{BASE} und akzeptieren
einen möglichen Zustand der Inkonsistenz. Außerdem sind NoSQL\hyp{}Datenbanken im
wesentlichen schemafrei\cite{NOSQLKRAMER}. Durch diesen Verzicht von
ACID\hyp{}Eigenschaften erreichen NoSQL\hyp{}Datenbanken eine hohe Leistung,
Skalierbarkeit und Flexibilität\cite[Siehe Seite 11]{NOSQLPERFORMANCE}.
\subsection*{Erwartete Daten und Wahl der geeigneten Datenbank}
\addcontentsline{toc}{subsection}{Erwartete Daten und Wahl der
geeigneten Datenbank}
Es werden folgende Informationen als Daten erwartet:
\begin{itemize}
    \item Ein sekundengenauer Zeitstempel.
    \item Der \gls{fqdn} oder die \gls{ip}\hyp{}Adresse des Messknotens
    \item Die Messdaten. Dies können sowohl boolesche Werte sein, als
        aber auch diverse Metriken wie Latenz oder CPU\hyp{}Auslastung.
\end{itemize}
Durch die strikte Abhängigkeit zwischen Messdaten und
ihrem sekundengenauen Zeitstempel fällt die Wahl auf eine \gls{tsdb}.
\glspl{tsdb} gehören zur Gruppe der NoSQL\hyp{}Datenbanken.
Folgende \gls{tsdb}\hyp{}Implementierungen stehen zur näheren Auswahl:
\begin{itemize}
    \item \emph{InfluxDB}\cite{INFLUXDB}
    \item \emph{OpenTSDB}\cite{OPENTSDB}
    \item \emph{Prometheus}\cite{PROMETHEUS}
\end{itemize}
\emph{InfluxDB} und \emph{OpenTSDB} sind Vertreter von
einer \gls{tsdb} und vielseitig einsetzbar. Interessant für dieses
Projekt ist jedoch \emph{Prometheus}. \emph{Prometheus} beschränkt sich
auf Monitoringdaten und erweitert eine \gls{tsdb} um ein ganzes
Ökosystem zum Überwachen von Servern, Container und Netzwerken.
Darunter fallen diverse Werkzeuge zum Sammeln von Daten, als auch
Software für das \emph{Alerting} von Administratoren im Fehlerfall.
Beispiele für Firmen die \emph{Prometheus} nutzen sind:
\emph{Soundcloud} (eine große Musikplattform für Künstler), \emph{Docker
Inc.} (die Firma hinter der Container\hyp{}Technologie Docker),
\emph{DigitalOcean} (ein weltweit erfolgreicher Hoster von virtuellen Maschinen
und Container), \emph{Jodel} (eine unter Studenten beliebte anonyme
Microblogging\hyp{}Plattform)\cite{PROMETHEUS}.

\section{Prothemeus}
Bei \emph{Prometheus} handelt es sich um keine reine \gls{tsdb}.
\emph{Prometheus} ist vielmehr eine auf Monitoringdaten spezialisierte
und mit diversen Komponenten erweiterbare Monitoringanwendung. In diesem
Kapitel wird der interne Aufbau von \emph{Prometheus}, dessen
Komponenten und deren Zusammenspiel näher erläutert. \emph{Prometheus}
wurde ursprünglich im Jahr 2012 von Mitarbeitern des Online-Musikdiensts
\emph{Soundcloud} entwickelt\cite{PROMETHEUS_OVERVIEW} und im Jahr 2015
als Open Source Software freigegeben. \emph{Prometheus} entstand aus der
Not heraus, hunderte von Mikroservices und tausende von Service\hyp{}Instanzen
in einem firmeninternen Container\hyp{}Cluster zu
überwachen\cite{PROMETHEUS_YOUTUBE}. Dieser Cluster war anfangs eine
komplette Eigenentwicklung\cite{PROMETHEUS_YOUTUBE}, wurde jedoch später gegen den
Cluster\hyp{}Scheduler Kubernetes ersetzt\cite{KUBERNETESSOUNDCLOUD}, welcher gängige
Container\hyp{}Technologien wie beispielsweise Docker
unterstützt\cite{KUBERNETES}. Seitdem setzt sich
\emph{Prometheus} besonders im Cloud\hyp{}Bereich durch und wurde 2016
sogar in die \emph{Cloud Native Computing Foundation}
aufgenommen\cite{CNCF}, einer Organisation zur Förderung von
Cloud\hyp{}Software im Open Source Bereich. Bekannte Mitglieder der
\emph{Cloud Native Computing Foundation} sind\cite{CNCFLANDSCAPE}:
\begin{itemize}
    \item Amazon AWS (Platinum Member)
    \item Microsoft Azure (Platinum Member)
    \item Alibaba Cloud (Platinum Member)
    \item Google Cloud (Platinum Member)
    \item Openstack (Open Source Member)
    \item RedHat (Platinum Member)
\end{itemize}

\subsection{Datenspeicherung in Prometheus}
Daten werden in \emph{Prometheus} als Tupel aus 64\hyp{}bit
Fließkommazahlen und einem Millisekunden genauen Zeitstempel
gespeichert\cite{PROMETHEUS_DATA_MODEL}, im
\emph{Prometheus}\hyp{}Umfeld auch Metrik genannt. Diese Tupel werden
mit einem Identifikator versehen. Dieser Identifikator besteht aus einem
eindeutigen Namen für die Metrik (zum Beispiel die Latenz in
Millisekunden) und einem Key\hyp{}Value\hyp{}Store (siehe
\autoref{lst:prometheusdataformat} Zeile 1; für ein Beispiel siehe Zeile
2). Die Keys werden auch \emph{Labels} genannt. Anhand dieser
\emph{Labels} ist es möglich, Daten einer Metrik zu filtern. Zwei
Stunden werden diese Metriken von \emph{Prometheus} standardmäßig
gesammelt bis sie, zusammen mit Metadaten und einer Indexdatei, in
Dateien in einem Verzeichnis abgelegt werden. Metadaten sind
Informationen über die gesammelten Daten, darunter fallen die
\emph{Labels}, welche beispielsweise Informationen zur Herkunft der
gesammelten Metrik beinhalten können. Die Indexdatei speichert die
Relation zwischen den Metriknamen und den Metriken, die gestückelt
abgelegt werden. Diese Stückelungen werden auch \emph{Chunks}
genannt\cite{PROMETHEUS_STORAGE} und ermöglichen eine Isolation
einzelner Transaktionen, vergleichbar mit dem ACID\hyp{} Paradigma bei
relationalen Datenbanken. Bevor Metriken gespeichert werden, legt
\emph{Prometheus} außerdem einen \gls{wal} an. Der \gls{wal} dient zur
Wiederherstellung von Daten nach einem Crash, beispielsweise ein
Stromausfall. In diesem Fall werden die zwischengespeicherten und noch
nicht umgesetzten Transaktionen auf Konsistenz geprüft und in die
\gls{tsdb} übertragen.  Gelöschte Daten werden nicht unwiderruflich
gelöscht, sondern in \emph{Tombstone}\hyp{}Dateien abgelegt.
\autoref{lst:prometheusstorage} zeigt eine solche Verzeichnisstruktur,
mit einzelnen \emph{Chunks}, \gls{wal}\hyp{}Elementen, Metadaten und
\emph{Tombstone}\hyp{}Dateien. Die \emph{Prometheus}\hyp{}Datenbank
arbeitet demnach dateibasiert.
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Prometheus Datenformat und
Beispiel},label={lst:prometheusdataformat}]
<metric name>{<label name>=<label value>, ...}
http_requests_total{service="service", server="www.tu-clausthal.de", env="production"}
\end{lstlisting}
\end{minipage}
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Beispiel für die Datenspeicherung in Prometheus},label={lst:prometheusstorage}]
./data/01BKGV7JBM69T2G1BGBGM6KB12
./data/01BKGV7JBM69T2G1BGBGM6KB12/meta.json
./data/01BKGV7JBM69T2G1BGBGM6KB12/wal
./data/01BKGV7JBM69T2G1BGBGM6KB12/wal/000002
./data/01BKGV7JBM69T2G1BGBGM6KB12/wal/000001
./data/01BKGTZQ1SYQJTR4PB43C8PD98
./data/01BKGTZQ1SYQJTR4PB43C8PD98/meta.json
./data/01BKGTZQ1SYQJTR4PB43C8PD98/index
./data/01BKGTZQ1SYQJTR4PB43C8PD98/chunks
./data/01BKGTZQ1SYQJTR4PB43C8PD98/chunks/000001
./data/01BKGTZQ1SYQJTR4PB43C8PD98/tombstones
./data/01BKGTZQ1HHWHV8FBJXW1Y3W0K
./data/01BKGTZQ1HHWHV8FBJXW1Y3W0K/meta.json
./data/01BKGTZQ1HHWHV8FBJXW1Y3W0K/wal
./data/01BKGTZQ1HHWHV8FBJXW1Y3W0K/wal/000001
./data/01BKGV7JC0RY8A6MACW02A2PJD
./data/01BKGV7JC0RY8A6MACW02A2PJD/meta.json
./data/01BKGV7JC0RY8A6MACW02A2PJD/index
./data/01BKGV7JC0RY8A6MACW02A2PJD/chunks
./data/01BKGV7JC0RY8A6MACW02A2PJD/chunks/000001
./data/01BKGV7JC0RY8A6MACW02A2PJD/tombstones
\end{lstlisting}
\end{minipage}

\subsection{Komponenten}
Die meisten \emph{Prometheus}\hyp{}Komponenten sind in der Sprache
\emph{Go} (auch bekannt als \emph{Golang}) geschrieben. \emph{Go} ist
eine von der Firma \emph{Google} entwickelte, kompilierbare
Programmiersprache, welche sich vorallem dadurch auszeichnet, dass sie
sicherer als die Programmiersprache \emph{C} ist und eine gute
Portierbarkeit aufgrund von statischen Binärdateien besitzt. Durch diese
Eigenschaften setzt sich \emph{Go} immer stärker, inbesondere im
Cloud\hyp{}Bereich und im \emph{Kubernetes}\hyp{}Umfeld, durch\cite{INFOWORLD}.
So schreibt der \emph{Go}\hyp{}Blog, dass \emph{Go} mittlerweile von
allen global agierenden Cloud\hyp{}Firmen eingesetzt wird, darunter
\emph{Amazon AWS}, \emph{Microsoft Azure} und \emph{Google
Cloud}\cite{GOBLOG}. Das
Branchenanalyseunternehmen \emph{RedMonk} listet eine Vielzahl von
\emph{Go}\hyp{}basierten Softwareprodukten darunter\cite{REDMONK}:
\begin{itemize}
    \item Docker
    \item Kubernetes (Der \emph{Amazon AWS} Dienst \emph{Amazon EKS} nutzt
        beispielsweise Kubernetes und Docker)
    \item gRPC
    \item fluentd
    \item Grafana
    \item Prometheus
\end{itemize}
Alle diese Softwareprodukte werden gefördert durch die \emph{Cloud
Native Computing Foundation}\cite{CNCFLANDSCAPE}.
\autoref{fig:prometheus} zeigt eine Übersicht über die wichtigsten
\emph{Prometheus} Komponenten.
\emph{Prometheus}' Kernkomponente ist der \emph{Prometheus Server}. Der \emph{Prometheus
Server} beinhaltet eine \gls{tsdb}, einen \gls{http}\hyp{}Server und ein
Modul zum Anfordern von Messdaten (im weiteren Verlauf
\emph{Retrieval} genannt). Der \gls{http}\hyp{}Server bietet eine grafische
Oberfläche via \gls{html} an, um Queries auf der \gls{tsdb} zu testen
und Graphen anzuzeigen. \autoref{fig:prometheuswebui} zeigt
die grafische Oberfläche mit einem Beispiel\hyp{}Query und Graphen.
Außerdem bietet der \gls{http}\hyp{}Server
eine \gls{restapi} an, an der die restlichen Komponenten andocken
können. Weitere Komponenten sind der \emph{Alertmanager}, das
\emph{Pushgateway}, die \emph{Exporter} und das bereits erläuterte und
im \emph{Prometheus Server} integrierte \emph{Prometheus Web UI}. Der
\emph{Alertmanager} ist verantwortlich für das Alarmieren der Benutzer,
wenn Messdaten vom Benutzer bestimmte Richtwerte überschreiten.
Unterstützt werden vom \emph{Alertmanager} direkt keine
Kommunikationswege. Viel mehr wird Fremdsoftware, beispielsweise für den
Mailtransfer, an den \emph{Alertmanger} angedockt. Das
\emph{Pushgateway} bietet einen statischen Bezugspunkt und Puffer, von dem der
\emph{Prometheus Server} mit dem \emph{Prometheus Retrieval}\hyp{}Modul Messdaten
abholen kann. Dies ist nützlich, wenn der \emph{Prometheus Server}, statt
die Messdaten abzuholen, die Messdaten zugeschoben bekommen soll. Diese Messdaten
werden zum \emph{Pushgateway} geschoben und \emph{Prometheus
Server} holt die Messdaten dort ab. Weitere Komponenten sind die
\emph{Exporter}. \emph{Exporter} sind Applikationen, welche
Messdaten vom Host über offene Schnittstellen sammeln und diese via einem
\gls{http}\hyp{}Server dem \emph{Prometheus Server} zur Verfügung
stellen. Die \emph{Exporter} exportieren demnach die gewonnenen Messdaten an das
\emph{Prometheus Retrieval}\hyp{}Modul, welches die Daten in die
\gls{tsdb} importiert. Es gibt eine Vielzahl von
\emph{Exportern} und die Zahl ist stetig steigend. Prominente Beispiele
für \emph{Exporter} sind:
\begin{itemize}
    \item \emph{Node Exporter}
    \item \emph{Blackbox Exporter}
\end{itemize}
\emph{Node Exporter} werden auf Hosts platziert und liefern allgemeine
Messdaten von diesem Host. Außerdem werden diverse Services, die auf dem
Host laufen, automatisch erkannt (auch bezeichnet als \emph{Service
Discovery}). Für das sammeln der Performance\hyp{} oder Messdaten greift
der \emph{Node Exporter} auf Schnittstellen im Kernel des
Betriebssystems zurück. Der Linux Kernel beispielsweise bietet
Statistiken und Performance\hyp{}Daten via Dateien in den Ordnern
\path{/sys/} oder \path{/proc/sys/} an. Der \emph{Node Exporter} liest
aus diesen Dateien und exportiert die gewonnenen Messdaten über den
eigenen \gls{http}\hyp{}Server an das \emph{Prometheus
Retrieval}\hyp{}Modul. Der \emph{Blackbox Exporter} baut ebenfalls einen
\gls{http}\hyp{}Server als Quelle für den \emph{Prometheus Server} auf,
aber Daten werden nicht über das lokale System  gesammelt. Stattdessen
werden von dem \emph{Blackbox Exporter} aus Tests an entfernten Servern
unternommen, wie beispielsweise die Erreichbarkeit von
\gls{dns}\hyp{}Servern oder \gls{smtp}\hyp{}Servern. Dazu sendet der
\emph{Blackbox Exporter} zum Beispiel \gls{dns}\hyp{}Pakete an einen
\gls{dns}\hyp{}Server und überprüft so, ob der \gls{dns}\hyp{}Server vom
\emph{Blackbox Exporter} aus erreichbar ist.  Alle Verbindungen zwischen
\emph{Prometheus} Komponenten, sind \gls{http}\hyp{}Anfragen einer
\gls{restapi}. Alle Komponenten sind statische ausführbare Binärdateien
(\emph{Executables}). Es existieren eine Vielzahl von anderen
\emph{Exportern} auf die hier nicht näher eingegangen wird. Eine
vollständige Liste findet sich auf der \emph{Prometheus} Webseite.
Darunter sind beispielsweise auch \emph{Exporter} um automatisch
Performance\hyp{}Daten aus der \emph{Amazon AWS} Cloud zu
exportieren\cite{PROMETHEUS_EXPORTERS}:
\begin{itemize}
    \item AWS ECS Exporter\cite{ECS_EXPORTER}
    \item AWS Health Exporter\cite{AWS_HEALTH}
    \item AWS SQS Exporter\cite{SQS_EXPORTER}
\end{itemize}
Dieses Beispiel unterstreicht die Einsatzfähigkeit und Adoption von
\emph{Prometheus} im Cloud\hyp{}Bereich.
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/prometheus.pdf}
    \caption{Interner Aufbau des Prometheus Server und dessen
    externe Komponenten}\label{fig:prometheus}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/prometheus_web_ui.png}
    \caption{Prometheus Web UI Beispiel, welches via Node Exporter
    gesammelte Messdaten in Form einer Zeitreihe darstellt}\label{fig:prometheuswebui}
\end{figure}
\subsection{PromQL}
\emph{Prometheus} besitzt zum Abfragen der Messdaten eine eigene
Query\hyp{}Language names \emph{PromQL}. \emph{PromQL} ist nicht
vergleichbar mit \gls{sql} und besitzt keinen relationalen Ansatz.
Stattdessen handelt es sich um eine Art Filtersprache, welche mit
booleschen Operatoren und diversen Funktionen ergänzt worden ist.
Anhand von \emph{PromQL} lassen sich einzelne Messdatensätze auswählen
und weiter auswerten oder filtern\cite{PROMETHEUS_PROMQL}. Dies ist insbesondere für die
grafische Darstellung der Messdaten relevant. Dazu unterstützt die
\emph{PromQL} vier verschiedene Datentypen\cite{PROMETHEUS_PROMQL}:
\begin{description}
    \item[Instant Vector] ein Set von Messdaten mit demselben
        Zeitstempel.
    \item[Range Vector] eine Auswahl von Messdaten über einen größeren
        Zeitraum.
    \item[Scalar] ein Fließkommawert.
    \item[String] ein Array von Chars (zurzeit von \emph{Prometheus} unbenutzt).
\end{description}
Der \emph{Instant Vector} besteht aus einem eindeutigen Namen, einer Metrik und
einem Key\hyp{}Value\hyp{}Store (siehe auch
\autoref{lst:prometheusdataformat}). Innerhalb eines \emph{Instant Vectors}
lassen sich reguläre Ausdrücke (Regex) anwenden. Ein regulärer Ausdruck ist
eine Zeichenkette, die der Beschreibung von Mengen von anderen Zeichenketten
mithilfe syntaktischer Regeln dient\cite{REGEXWIKI}. Für reguläre Ausdrücke
verwendet \emph{PromQL} die von der Firma \emph{Google} entwickelte Bibliothek
\emph{RE2}\cite{RE2}. Wird dieser \emph{Instant Vector} um einen \emph{Range
Selector} erweitert, handelt es sich um einen \emph{Range Vector}. Ein
\emph{Range Selector} ist eine in eckigen Klammern geschriebene Zeitangabe für
einen \emph{Instant Vector}. Der \emph{Range Selector} wird dem \emph{Instant
Vector} nachträglich angefügt.  Zusätzlich lassen sich diese Vektoren durch
weitere Funktionen und Operatoren beeinflussen. So bietet \emph{PromQL} eine
Vielzahl von weiteren Funktionen, um beispielsweise den Durchschnitt über einen
\emph{Range Vector} zu bilden\cite{PROMETHEUS_PROMQL_FUNCTIONS}. \emph{PromQL}
ist eine harte Voraussetzung um die \emph{Prometheus GUI} zu bedienen, da
spätere Queries zur Visualisierung der Messdaten alle in \emph{PromQL}
geschrieben werden.
\section{Grafana}
Da die \emph{Prometheus Web UI} eingeschränkt mit Graphen und
Visualisierung der Daten umgehen kann und eher eine Testplattform für
\emph{PromQL-Anfragen} darstellt, wird für den Zweck der
Visualisierung eine weitere Plattform verwendet:
\emph{Grafana}\cite{GRAFANA}. \emph{Grafana} zeichnet sich durch eine
hohe Erweiterbarkeit, vollen \emph{Prometheus}\hyp{}Support und einer
Vielzahl an Konfigurationsmöglichkeiten aus. Der Transport der Daten von
\emph{Prometheus} zu \emph{Grafana} findet via \gls{restapi} und der, von
\emph{Prometheus} unterstützten Query\hyp{}Language \emph{PromQL} statt
(\autoref{fig:prometheusgrafana} zeigt \emph{Grafanas} Platz in der
\emph{Prometheus}\hyp{}Landschaft).
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/prometheus_grafana.pdf}
    \caption{Eingliederung von Grafana in Prometheus}\label{fig:prometheusgrafana}
\end{figure}
Ein weiteres Merkmal von \emph{Grafana} sind \emph{Grafanas}
Provisionierungseigenschaften. Durch diverse Einstellungen in
\emph{Grafanas} Konfigurationsdateien und Umgebungsvariablen lässt sich
\emph{Grafana} gut in IaaS\hyp{}Umgebungen (\gls{iaas})
ausrollen (Umgebungsvariablen sind temporäre oder permanente Variablen im
Betriebssystem)\cite[Beispiel zum Deployment von Grafana in Amazon
AWS]{GRAFANA_AWS_DEPLOYMENT}. \emph{Grafana} besitzt beispielsweise nativen
Support für \emph{Amazon AWS Cloudwatch}\cite{GRAFANA_CLOUDWATCH}.
\section{Konfigurationsmanagement}
In der Vergangenheit wurden Rechnersysteme zunehmend vertikal skaliert,
das heißt, die technischen Ressourcen des einzelnen Rechnersystems wurden angehoben
(beispielsweise der \gls{ram}). Es wurde jedoch zunehmends
festgestellt, dass diese Vorgehensweise nicht nur sehr kostenintensiv
ist, sondern auch nichts zur Ausfallsicherheit beiträgt. Die
Antwort auf dieses Problem ist horizontale Skalierung. Bei horizontaler
Skalierung werden weitere Instanzen von Rechnersystemen der Anwendung
hinzugefügt. Dieser Ansatz ermöglicht ausfallsichere und äußerst performante
Infrastrukturen, vorausgesetzt das zu berechnende Problem ist horizontal
skalierbar. Dies ist der Fall bei voneinander unabhängigen Input. Horizontale
Skalierung hat allerdings auch andere Ansprüche. So wächst die Anzahl der zu
betreuenden Systemen rapide an.  Damals wurden solche Systeme noch von Menschen
gepflegt, konfiguriert und installiert. Durch die schiere Masse der neuen
Systeme ist das nun unmöglich und zu kostenintensiv geworden.  Daraufhin wurden
Anwendungen ins Leben gerufen, welche die Arbeiten an mehreren Systemen
vereinfachen sollen. Die zwei Leitbegriffe für solche Anwendungen sind
\emph{Orchestrierung} und \emph{Konfigurationsmanagement}. Bei der
\emph{Orchestrierung} geht es darum, eine hohe Anzahl von Systemen oder Services zu steuern,
also gewisse Aufgaben auf den jeweiligen Systemen anzustoßen.
\emph{Konfigurationsmanagement} dagegen umfasst die Installation und
Konfiguration von Software auf Systemen, im englischsprachigen Raum zusammengefasst
\emph{Deployment} genannt. Da im Rechenzentrum der \gls{tuc} zurzeit das auf
der Programmiersprache \emph{Python} basierende Orchestrierungs\hyp{} und
Konfigurationsmanagementwerkzeug \emph{Ansible} dominiert, beschränkt sich diese
Arbeit auf die Verwendung von \emph{Ansible}. Es gibt jedoch andere populäre
Werkzeuge. Darunter fallen beispielsweise \emph{Puppet}, \emph{Saltstack} und
\emph{Chef}.
\section{Ansible}
Die Entwicklung an \emph{Ansible} startete im Februar 2012 unter Michael
DeHaan als Open Source Anwendung\cite{ANSIBLE_ORIGINS}. Ein Jahr
später ging aus dem Projekt die Firma \emph{AnsibleWorks} hervor, welche
neben der Entwicklung von Ansible professionellen Support für die
Software anbietet und auch an einer webbasierten Benutzerschnittstelle
namens \emph{Tower} arbeitet\cite{ANSIBLEWORKS}. Im Jahr 2015 wurde die
Firma durch den großen Linux\hyp{}Dienstleister \emph{Red Hat}
übernommen\cite{ANSIBLEREDHAT}. Wenig später begann die
Open Source Gemeinschaft damit, \emph{Ansible} um unzählige Module zu
erweitern und so den Funktionsumfang zu vergrößern.
\subsection{Funktionsweise}
\emph{Ansible} arbeitet mit Templates zur Orchestrierung und
Konfiguration von Programmpaketen auf Rechnern und Rechner
\hyp{}Konfigurationsdateien. Die Auszeichnungssprache für die
Datenstrukturen, welche die Templates befüllen, heißt \gls{yaml} (oft
auch nur mit YML abgekürzt). \gls{yaml} zeichnet sich durch für Menschen
einfache Lesbarkeit und die Möglichkeit, Kommentare einzufügen,
aus\cite{YAML_WIKI}. Außerdem besteht die Möglichkeit, in der
Programmiersprache \emph{Python} geschriebene Module mit in \gls{yaml}
aufgeschrieben Anweisungen zu nutzen. Diese Module sind quelloffen und
werden von einer Community um \emph{Ansible} und der Firma \emph{Red
Hat} gepflegt. Die zu pflegenden Systeme, nachfolgend nur noch als
Zielsysteme bezeichnet, können in \emph{Ansible} in Gruppen 
organisiert werden.
Als Eingabe für \emph{Ansible} dienen speziell auf \emph{Ansible}
zugeschnittene \gls{yaml}\hyp{}Dateien. In diesen
\gls{yaml}\hyp{}Dateien werden Konfigurationsparameter als
\gls{yaml}\hyp{}Datenstrukturen erfasst. \emph{Ansible} nimmt
anschließend diese \gls{yaml}\hyp{}Datenstrukturen als Eingabe und führt
\emph{Python} oder Kommandos auf der Kommandozeile auf dem Zielsystem
aus, um die spezifizierten
Konfigurationsparameter auf dem Zielsystem einzupflegen. Je nach Gruppe oder
Name des Zielsystems lassen sich unterschiedliche
Konfigurationsparameter in \emph{Ansible} laden.
Auf diese Weise können komplexe Gebilde von Software\hyp{}Systemen und
Hardware\hyp{}Konfigurationsparametern abgebildet
und Gemeinsamkeiten in der Orchestrierung und Konfiguration effektiv ausgenutzt werden.
Um Konfigurationsdateien für gängige Software wie beispielsweise
\emph{OpenSSH} oder Webserver wie \emph{Apache2} mit \emph{Ansible}
dynamisch zu generieren wird die auf \emph{Python} basierende Template\hyp{}Engine
\emph{Jinja2} benutzt\cite{JINJA2}. Durch \emph{Jinja2} lassen sich komplexe
Konfigurationsdateien von Software auf den Zielsystemen in Form von Templates
ausdrücken. Das Ziel ist, ein Template für mehrere
Konfigurationsvarianten einer Konfigurationsdatei zu haben. Diese
Templates werden mit Datenstrukturen in \gls{yaml} erweitert.
Um die Orchestrierung oder Konfiguration zu starten, reicht es, wenn sich
\emph{Ansible} auf dem System, welches zur Pflege anderer Rechner und
deren Software eingesetzt werden
soll, befindet. Nachfolgend wird dieses System als Managementsystem
bezeichnet. Das Managementsystem verbindet sich zu den einzelnen
Zielsystemen via dem Protokoll \gls{ssh} und arbeitet die auf dem
Managementsystem befindlichen Aufgaben für das Zielsystem in Form von
\gls{yaml}\hyp{}Dateien ab. Dies geschieht durch
die Verwendung der Sprache \emph{Python} oder einfach durch Kommandos 
der Kommandozeile der Zielsysteme. Durch
diese Vorgehensweise arbeitet \emph{Ansible} \emph{agentless}, also ohne
einen Agenten auf dem Zielsystem. Dies
ermöglicht den Einsatz von \emph{Ansible} auch auf Systemen mit
eingeschränkter Leistungsfähigkeit (beispielsweise Switches oder Router).
Außerdem arbeitet \emph{Ansible} idempotent, dadurch lassen sich
Aufgaben beliebig oft ausführen und führen immer zum gleichen Ergebnis.
Dies setzt allerdings voraus, das \emph{Ansible} korrekt konfiguriert
wurde.
\subsection{Organisationsstruktur}
\emph{Ansible} hat eine definierte Struktur zur Speicherung der
Konfigurationsdateien, Aufgaben, Datenstrukturen für die Templates und
Templates. Dabei bedient sich \emph{Ansible} bei Synonymen aus der
Filmbranche, um den Einstieg in das Werkzeug zu erleichtern.
\emph{Playbooks} (Deutsch: Drehbücher) werden verwendet, um einzelnen
Hosts \emph{Roles} (Deutsch: Rollen) zuzuweisen. Die nötigen Daten zum
Abspielen dieser Rollen befinden sich im \emph{Inventory} (Deutsch:
Inventar), zu dem die Hosts ebenfalls gehören. 
\autoref{lst:ansiblestructure} zeigt die Organisationsstruktur eines
solchen \emph{Ansible}\hyp{}Projekts. Die Datei \emph{ansible.cfg} ist die
Konfigurationsdatei für \emph{Ansible} und definiert den Pfad zum
Inventar, den Rollen und den \emph{Playbooks}. Die Datei \emph{hosts}
beinhaltet eine Auflistung aller Zielsysteme mit Eingliederung in
Gruppen und Subgruppen. Auf diese Weise lassen sich komplexe
Installationen aus Zielsystemen hierarchisch gliedern und
Gemeinsamkeiten festmachen. In den Dateien \emph{group\_vars} und
\emph{host\_vars} werden die eigentlichen Daten für die Gruppen und
einzelnen Hosts vermerkt. Der Ordner \emph{playbooks} umfasst alle
\emph{Playbooks}. Im Beispiel ist das \emph{Playbook}
\emph{configure\_hosts} abgebildet. Es handelt sich um eine \gls{yaml}\hyp{}Datei.
Der Ordner \emph{roles} beinhaltet die Rolle \emph{hello\_world} mit den
Unterordnern \emph{defaults}, \emph{files}, \emph{tasks} und
\emph{templates}. Im Ordner \emph{defaults} befinden sich
Standardbelegungen, mit denen Variablen initialisiert werden. Der Ordner
\emph{files} umfasst statische Dateien, welche auf den Zielsystemen
installiert werden sollen. Die letzten beiden Ordner \emph{tasks} und
\emph{templates} umfassen die eigentliche Logik zur Konfiguration (die
abzuarbeitenden Aufgabenschritte) und \emph{Jinja2}\hyp{}Templates. Um
eine Konfiguration zu starten, würde der Administrator das
\emph{Playbook} {configure\_hosts.yml} mit dem Kommando
\emph{ansible-playbook playbooks/configure\_hosts.yml} aufrufen.
\emph{Ansible} würde daraufhin das zu den Hosts gehörende Inventar
heraussuchen und die in den Rollen und Templates vorhandenen Variablen
durch Werte aus den \emph{host\_vars} oder \emph{group\_vars} ersetzen.
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Organisationsstruktur eines Ansible
Projekts},label={lst:ansiblestructure}]
.
|-- ansible.cfg
|-- group_vars
|-- hosts
|-- host_vars
|-- playbooks
|   `-- configure_hosts.yml
`-- roles
    `-- hello_world
        |-- defaults
        |   `-- main.yml
        |-- files
        |   `-- static_file.txt
        |-- tasks
        |   `-- main.yml
        `-- templates
            `-- configuration.j2
\end{lstlisting}
\end{minipage}
\subsection{Dateistruktur}
\emph{Ansible} nutzt als zentrale Dateistruktur die Auszeichnungssprache
\gls{yaml} und ergänzt diese durch Elemente der
Python\hyp{}Template\hyp{}Engine \emph{Jinja2}. \gls{yaml} unterstützt
gängige Datenstrukturen wie in etwa Listen und Wörterbücher
(Dictionaries) und Kombinationen davon sowie Kommentare. Höhere
Logikebenen werden durch \gls{yaml} und \emph{Jinja2} erreicht (If/Else,
For\hyp{}Schleifen, While\hyp{}Schleifen etc). Dadurch ist \emph{Jinja2}
in der Theorie Turing-vollständig. \autoref{lst:ansiblessh} ist ein
Beispiel für die Dateistruktur eines \emph{Ansible Tasks}. Der
\emph{Task} verteilt \gls{ssh}\hyp{}Schlüssel auf eine beliebige Anzahl
an Zielsystemen.  \gls{yaml}\hyp{}Dateien beginnen stets mit drei
Querstrichen und enden mit drei Punkten.  Mittlerweile ist dies jedoch
optional und \emph{Ansible} erkennt \gls{yaml}\hyp{}Dateien ohne diese
Markierungen (meistens an der Dateiendung \emph{.yml}). \emph{Ansible
Tasks} bestehen aus einer Liste von Dictionaries. Jeder Listeneintrag
stellt eine Aufgabe dar, welche \emph{Ansible} abzuarbeiten hat.
Initialisiert wird ein solcher Eintrag mit dem Namen der Aufgabe und
einer Abfolge von weiteren Schlüssel\hyp{}Wert\hyp{}Paaren. Bei den
Paaren handelt es sich um fest definierte und in Python verfasste
\emph{Ansible Module}. In dem Beispiel \autoref{lst:ansiblessh} sind
dies die Module \emph{file} und \emph{template}. Das
\emph{file}\hyp{}Modul dient zum Erstellen von Ordnern, Platzieren von
statischen Dateien oder Anlegen von Dateien mit statischen Inhalt.
Diesem Modul werden eine Anzahl von Parametern übergeben. In diesem Fall
ein Pfad auf einem Linux\hyp{}System, der gewollte Zustand
\emph{directory} sowie ein Besitzer, eine Gruppe und eine Konfiguration
der Dateiberechtigung. \emph{0700} bedeutet volle
Lese\hyp{}, Schreib\hyp{} und Ausführrechte für den Besitzer der Datei
oder des Ordners. Dem zur Folge erstellt diese Zeile einen Ordner mit
den im vorherigen Satz erwähnten Dateirechten und dem Besitzer
\emph{root}. Das \emph{template}\hyp{}Modul hingegen liest ein lokales
\emph{Jinja2}\hyp{}Template ein, wertet dieses aus und hinterlegt es auf
dem Zielsystem mit den Dateirechten \emph{0600} (Schreib\hyp{} und
Leserechte) für den Benutzer \emph{root} in dem Pfad
\path{/root/.ssh/authorized\_keys}. \autoref{lst:ansiblejinja2}
zeigt das zum \emph{Task} gehörende \emph{Jinja2}\hyp{}Template. Die
erste Zeile im Template konfiguriert \emph{Jinja2} so, dass alle
Leerzeichen oder Tabs am Anfang einer Zeile gelöscht werden. In Zeile 2
ist der Anfang einer For\hyp{}Schleife in \emph{Jinja2}\hyp{}Syntax
abgebildet. In dieser For\hyp{}Schleife wird über die Liste
\emph{root\_ssh\_keys} iteriert und jede Iteration wird der Variable
\emph{user} als Wert zugewiesen. Der vertikale Strich startet einen
Filter. In diesem Fall der Filter \emph{sort}. Die Liste wird durch
diesen Filter vor dem Schleifendurchlauf sortiert. Der horizontale
Strich am Ende der Anweisung weist die Schleife an, alle Leerzeichen oder
Tabs am Ende der generierten Zeile zu entfernen. Innerhalb der
For\hyp{}Schleife wird die Funktion \emph{lookup} aufgerufen, welche
eine Information auf dem lokalen Dateisystem einliest. Als Argument für
die Funktion wird angegeben, dass die Information aus einer Datei
bezogen werden soll und der Pfad zu dieser Datei wird aus der
\emph{user}\hyp{}Schleifenvariable und dem String \emph{../pubkeys/}
zusammengebaut. Die letzte Zeile beendet die Schleife. Die Eingabe für
dieses Template ist in \autoref{lst:ansiblessh2} abgebildet. 
\autoref{lst:ansiblessh2} wird dazu in den Ordnern \emph{group\_vars}
oder \emph{host\_vars} abgelegt und definiert die Liste
\emph{root\_ssh\_keys} mit den beiden Einträgen \emph{chris.pub} und
\emph{test.pub}
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Beispiel eines Ansible Tasks},label={lst:ansiblessh}]
---

- name: ensure /root/.ssh exists
  file: path=/root/.ssh state=directory owner=root group=root mode=0700

- name: add authorized keys for root
  template: src=authorized_keys.j2 dest=/root/.ssh/authorized_keys owner=root group=root mode=0600

...
\end{lstlisting}
\end{minipage}
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Beispiel eines Jinja2-Templates},label={lst:ansiblejinja2}]
#jinja2: lstrip_blocks: True
{% for user in root_ssh_keys | sort -%}
	{{ lookup('file', '../pubkeys/' + user) }}
{% endfor %}
\end{lstlisting}
\end{minipage}
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Daten für das Jinja2-Template Beispiel},label={lst:ansiblessh2}]
---

root_ssh_keys:
  - chris.pub
  - test.pub

...
\end{lstlisting}
\end{minipage}
\subsection{Sicherheit}
\emph{Ansible} nutzt für alle Verbindungen das
Fernadministrationsprotokoll \glsfirst{ssh}. Dadurch ist die Integrität,
Sicherheit und Authentizität der übertragenen Daten sichergestellt.
\gls{ssh} verwendet dazu ein asynchrones Verschlüsselungsverfahren. Auf
dem Client wird ein Schlüsselpaar generiert. Dies erfolgt durch moderne
kryptografisch sichere Verfahren wie \gls{rsa} oder elliptischen Kurven.
Der öffentliche Schlüssel wird auf dem Zielsystem platziert, meist
geschieht dies durch vorheriges Einloggen auf dem Zielsystem mit einem
Passwort. Bei der Verbindung vom Client zum Zielsystem wird durch das
asynchrone Verfahren die Authentizität des Servers und des Clients
bestätigt und der Zugriff gewährt. Um bestimmte Daten auch auf dem
lokalen Dateisystem des Managementsystems zu verschlüsseln wird
\emph{Ansible\hyp{}Vault} verwendet. \emph{Ansible\hyp{}Vault} ist ein
Mechanismus, um Dateien mit einem symmetrischen Schlüssel zu
verschlüsseln. Als Algorithmus wird das Verschlüsselungsverfahren
\gls{aes} verwendet\cite{ANSIBLEVAULT}.
\section{Wahl der Hardware}
Da das zu entwickelnde Software\hyp{}System horizontal skalierbar sein soll (siehe
Nicht-Funktionale Anforderung 4 (NFA4)), soll das System auch günstig in
der Anschaffung sein (NFA2). Dem kommt zugute, dass die Software auf den
Messpunkten nicht viele Hardware\hyp{}Ressourcen benötigt. Es wird
lediglich genug Speicher für ein kleines Linux mit einigen
Kernkomponenten wie \emph{OpenSSH} und der eigentlichen Software für die
Messpunkte benötigt. Durch die niedrigen Beschaffungskosten fallen somit
bereits größere Computersysteme in Form von Server\hyp{}Racks oder
Tower\hyp{}PC\hyp{}Einheiten weg. Übrig bleibt die Klasse der
Einplatinenrechner. Einplatinenrechner erfreuen sich in den letzten
Jahren immer größerer Beliebtheit und liegen in einem Budget\hyp{}Level
von \EUR{10} bis \EUR{100}. Nachfolgend werden die zwei Einplatinenrechner
\emph{Raspberry Pi 3B+} und \emph{Odroid XU4} verglichen.
Der \emph{Raspberry Pi 3B+} hat die folgenden Spezifikationen\cite{RASPI}:
\begin{itemize}
    \item CPU: Broadcom BCM2837B0 quad-core A53 (ARMv8) 64-bit @ 1.4GHz
    \item GPU: Broadcom Videocore-IV
    \item RAM: 1 GB LPDDR2 SDRAM
    \item Netzwerk: Gigabit Ethernet (via USB channel), 2.4GHz and 5GHz 802.11b/g/n/ac Wi-Fi
    \item Bluetooth: Bluetooth 4.2, Bluetooth Low Energy (BLE)
    \item Speicher: Micro-SD
    \item GPIO: 40-pin GPIO header
    \item Anschlüsse: HDMI, 3.5 mm analogue audio-video jack, 4x USB 2.0, Ethernet, Camera Serial Interface
        (CSI), Display Serial Interface (DSI)
    \item Dimensionen: 82 mm x 56 mm x 19.5 mm, 50g Gewicht
    \item Preis: 32.88 britische Pfund\cite{RASPI_PRICE} (\EUR{36.45} Stand: 2018-09-03)
\end{itemize}
Im Gegenzug dazu die \emph{Odroid XU4} Spezifikationen\cite{ODROID}:
\begin{itemize}
    \item CPU: Samsung Exynos5422 Cortex™-A15 2Ghz and Cortex™-A7 Octa core CPUs
    \item GPU: Mali-T628 MP6(OpenGL ES 3.1/2.0/1.1 and OpenCL 1.2 Full profile)
    \item RAM: 2 GB LPDDR3 RAM PoP stacked
    \item Netzwerk: Gigabit Ethernet, kein WLAN
    \item Bluetooth: nicht vorhanden
    \item Speicher: Micro-SD
    \item GPIO: 30-pin GPIO header\cite{ODROID_GPIO}
    \item Anschlüsse: 2x USB 3.0, 1x USB 2.0, HDMI 1.4a
    \item Dimensionen: 83 mm x 58 mm x 20 mm
    \item Preis: 59 US\hyp{}Dollar\cite{ODROID_PRICE} (\EUR{50,80} Stand: 2018-09-03)
\end{itemize}
Der \emph{Raspberry Pi 3B+} überzeugt zwar durch einen niedrigeren
Preis, jedoch ist es für den \emph{Raspberry Pi 3B+} unmöglich die
theoretische Leistung von 1 Gbit/s zu erreichen, da der Prozessor nur
mit einer USB 2.0 Schnittstelle zu der Gigabit Schnittstelle verbunden
ist. Deshalb limitiert sich die praktische Geschwindigkeit auf maximal
300 Mbit/s\cite{RASPI}. Da das Projekt jedoch eine native 1 Gigabit
Schnitstelle voraussetzt (siehe \autoref{table:FA8}), fällt die Wahl auf
den \emph{Odroid XU4}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/odroid.jpg}
    \caption{Darstellung eines Odroid XU4}\label{fig:odroid}
\end{figure}
\chapter{Projektumsetzung}
Die folgenden Unterkapitel umfassen die Umsetzung eines ersten
Prototyps. Dazu gehören das Erfüllen der Anforderungen, das Aufsetzen
eines ersten Test- und Entwicklungssystem, die Versionsverwaltung
und der erste Prototype.
\section{Erfüllung der Anforderungen}
Nachfolgend sind die evaluierten Anforderungen und deren Umsetzung
beschrieben.
\tabulinesep=1.2mm
\begin{center}
    \begin{tabu}{X[-1] X}
\toprule
\textbf{Anforderung} & \textbf{Umsetzung}  \\
\midrule
        \hyperref[table:FA1]{\textbf{FA1}} & Die Überwachung von \emph{DNS} erfolgt durch den \emph{Prometheus} \emph{Blackbox-Exporter}. \\
        \hyperref[table:FA2]{\textbf{FA2}} & Die Überwachung von \emph{HTTP} erfolgt durch den \emph{Prometheus} \emph{Blackbox-Exporter}. \\
        \hyperref[table:FA3]{\textbf{FA3}} & Die Überwachung von \emph{HTTPS} erfolgt durch den \emph{Prometheus} \emph{Blackbox-Exporter}. \\
        \hyperref[table:FA4]{\textbf{FA4}} & \emph{Prometheus} ist zum Stand vom 01.09.2018 nicht in der Lage \emph{CIFS} zu überwachen. Um die Anforderung zu erfüllen, wird der \emph{Prometheus} \emph{Node-Exporter} benutzt und die darunter liegende Bibliothek \emph{ProcFS} um Funktionalität erweitert. \\
        \hyperref[table:FA5]{\textbf{FA5}} & Die Performance-Daten werden in einer \emph{TSDB} in \emph{Prometheus} gespeichert. \\
        \hyperref[table:FA6]{\textbf{FA6}} & Die Grafische Aufbereitung erfolgt durch die Plattform \emph{Grafana}. \\
        \hyperref[table:FA7]{\textbf{FA7}} & Die Bandbreitenmessung erfolgt durch den \emph{Prometheus} \emph{Node-Exporter}. \\
        \hyperref[table:FA8]{\textbf{FA8}} & Wird erfüllt über die \emph{Odroid XU4} Plattform. \\
        \hyperref[table:FA9]{\textbf{FA9}} & Das System bleibt kontrollierbar durch den Einsatz von \emph{Ansible}. \\
\bottomrule
    \end{tabu}
    \captionof{table}{Umsetzung der funktionalen Anforderungen}
    \label{table:mapping1}
\end{center}
\tabulinesep=1.2mm
\begin{center}
    \begin{tabu}{X[-1] X}
\toprule
\textbf{Anforderung} & \textbf{Umsetzung}  \\
\midrule
        \hyperref[table:NFA1]{\textbf{NFA1}} & Das für das Deployment verwendete Projekt \emph{Ansible} benutzt \emph{Python}. Für die Erweiterung um \emph{CIFS}\hyp{}Support ist nur die Verwendung der Programmiersprache \emph{Golang} möglich. \\
        \hyperref[table:NFA2]{\textbf{NFA2}} & Die niedrigen Beschaffungskosten sind durch die Wahl der \emph{Odroid XU4} Plattform gegeben. \\
        \hyperref[table:NFA3]{\textbf{NFA3}} & Die für das Deployment benötigten Passwörter werden mit gängigen Verschlüsselungsverfahren in \emph{Ansible-Vault}  hinterlegt. Die Verbindungen zu den Zielsystemen finden verschlüsselt und mit einer Authentizitätsprüfung statt. \\
        \hyperref[table:NFA4]{\textbf{NFA4}} & Das System ist horizontal skalierbar durch den Einsatz von \emph{Prometheus} als Cluster einsetzbare Datenbank und \emph{Ansible} als Automatisierungswerkzeug zum Deployment. \\
\bottomrule
    \end{tabu}
    \captionof{table}{Umsetzung der nicht-funktionalen Anforderungen}
    \label{table:mapping2}
\end{center}
\section{Test- und Entwicklungsumgebung}
Da die benötigte Hardware zu Projektanfang noch nicht zur Verfügung
stand, galt es eine Test\hyp{} und Entwicklungsumgebung aufzubauen. Als
Grundlage für diese Test\hyp{} und Entwicklungsumgebung dient die
Software \emph{Vagrant}\cite{VAGRANT} der Firma \emph{Hashicorp}. \emph{Hashicorp} ist
eine in San Fransisco im Jahr 2012 gegründete Firma, welche sich auf den
Einsatz und Entwicklung von Open Source Software
spezialisiert\cite{HASHICORP}. Der Fokus der Firma liegt auf
Automatisierung, Deployment und der Orchestrierung von
Softwaresystemen in der Cloud. Die Software ist auch ohne
eine Cloud\hyp{}Umgebung anwendbar. Für die Test\hyp{} und
Entwicklungsumgebung wird nur auf das Produkt \emph{Vagrant} der Produktpalette von
\emph{Hashicorp}, gemeinhin auch als \emph{Hashicorp-Toolchain} bekannt,
zurückgegriffen. Das Werkzeug \emph{Vagrant} bietet eine
Abstraktionsschicht über die gängigen Virtualisierungsplattformen wie in
etwa \emph{Virtualbox}, \emph{VMWare}, \emph{Qemu} aber auch über
Cloud\hyp{}Plattformen wie zum Beispiel \emph{Google Cloud},
\emph{Amazon AWS} und \emph{Openstack}. Die von \emph{Vagrant}
unterstützten Virtualisierungsplattformen werden in \emph{Vagrant}
\emph{Provider} genannt. \emph{Vagrant} selbst ist eine
statisch gebaute aus \emph{Golang} kompilierte Binärdatei. Dies
erleichtert das Deployment auf allen gängigen Systemen.
Eine Abhängigkeit zu Linux ist deshalb
nicht gegeben, \emph{Vagrant} ist auch auf \emph{Windows} oder
\emph{MacOS} einsetzbar\cite{VAGRANT}. Außerdem
zeichnet sich \emph{Vagrant} durch eine enge Verzahnung mit
Automatisierungswerkzeugen aus, darunter auch
\emph{Ansible}\cite{VAGRANT_DOCS}. \emph{Vagrant} abstrahiert die
Virtualisierungsplattform über einen \emph{Ruby}\hyp{}Interpreter. Es
wird eine Datei in der Programmiersprache \emph{ruby} angelegt, über
welche die Anzahl der virtuellen Maschinen und deren Orchestrierung
organisiert wird. Diese \emph{Ruby}\hyp{}Datei wird von \emph{Vagrant}
eingelesen und \emph{Vagrant} dockt über die APIs an die jeweilige
Virtualisierungsplattform an und startet die virtuellen Maschinen. Der Dateiname
einer solchen Datei ist immer \emph{Vagrantfile}. \emph{Vagrantfiles}
sind austauschbar und ermöglichen so ein reproduzierbares und
automatisiertes Aufsetzen einer Testumgebung in wenigen Schritten. Ein
weiterer Grund für den Einsatz von \emph{Vagrant} ist die Vielzahl an
fertigen \emph{Images} (Speicherabbildern von Betriebssystemen und
weiterer Software). Anstatt
ein benötigtes Betriebssystem manuell in einer VM zu installieren, ist es
möglich, direkt über \emph{Vagrant} fertige \emph{Images} für gängige
Linux\hyp{}Distributionen zu ziehen. Dies beschleunigt den
Entwicklungsprozess ungemein, da der Entwickler sich voll auf die
Anwendung konzentrieren kann und seine Zeit nicht mit dem Installieren
von Betriebssystemen verschwendet. \autoref{lst:vagrantfile} zeigt
die Datei namens \emph{Vagrantfile} zum Erzeugen der Test\hyp{} und
Entwicklungsumgebung. In Zeile 1 wird eine For\hyp{}Schleife erzeugt,
welche 2 virtuelle Maschinen erzeugt. Die erste virtuelle Maschine trägt
den Namen \emph{Puppetmaster} (Zeile 2) und wird mit einem \emph{Ubuntu
Bionic Beaver} (\emph{Ubuntu LTS 18.04}) konfiguriert (Zeile 3). \emph{Ubuntu} ist
ein Linux\hyp{}Derivat von der bekannteren Linux\hyp{}Distribution
\emph{Debian}. In Zeile 4 wird ein privates Netzwerk für die virtuelle
Maschine erzeugt. Die virtuelle Maschine bekommt demnach die
IP\hyp{}Adresse \textbf{192.168.33.10}. In den Zeilen 6 bis
einschließlich Zeile 8 wird eine zweite VM erzeugt und konfiguriert mit
dem Namen \emph{Node1}, demselben Betriebssystemabbild und der
IP\hyp{}Adresse \textbf{192.168.33.11}. Die virtuellen Maschinen liegen
beide im selben Netzwerk, damit sie sich gegenseitig erreichen können.
Sie sind via einer \emph{Bridge} mit dem Host\hyp{}Betriebssystem verbunden.
Anhand dieser \emph{Vagrantfile} ist es möglich, die Testumgebung auf jedem System zu
starten, auf dem \emph{Vagrant} installiert ist. Dazu muss lediglich die
\emph{Vagrantfile} in einen Ordner abgelegt werden und über die
Kommandozeile \emph{Vagrant} gestartet werden. Dies geschieht über das
Kommando \emph{vagrant up}. \emph{Vagrant} lädt durch diesen Befehl das
benötigte Betriebssystemabbild herunter, konfiguriert die virtuellen
Maschinen mit der in \emph{Vagrant} vorgegebenen
Standard\hyp{}Virtualisierungsplattform \emph{Virtualbox} und
konfiguriert das Netzwerk für die Maschinen. Wenn die Maschinen
gestartet sind, kann über den Befehl \emph{vagrant ssh <Name der vm>} auf
eine der virtuellen Maschinen über das Protokoll \gls{ssh} zugegriffen werden.
Durch diese Vorgehensweise lässt sich die Testumgebung später auf jeden
anderen Hypervisor mit Emulator übertragen. Über den Befehl
\emph{vagrant snapshot} lässt sich ein inkrementelles \emph{Image} der
virtuellen Maschine anlegen. Dieses \emph{Image} dient als Sicherung.
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Vagrantfile der Test- und Entwicklungsumgebung},label={lst:vagrantfile}]
Vagrant.configure("2") do |config|
  config.vm.define "Puppetmaster" do |Puppetmaster|
    Puppetmaster.vm.box = "ubuntu/bionic64"
    Puppetmaster.vm.network "private_network", ip: "192.168.33.10"
  end
  config.vm.define "Node1" do |Node1|
    Node1.vm.box = "ubuntu/bionic64"
    Node1.vm.network "private_network", ip: "192.168.33.11"
  end
end
\end{lstlisting}
\end{minipage}
Bei der Anwendung des Befehls \emph{vagrant ssh} wird eine
Implementierung des \gls{ssh}\hyp{}Protokolls innerhalb \emph{Vagrants} benutzt. Um
die globale \gls{ssh}\hyp{}Implementierung zu verwenden, ist es nötig, den
von \emph{Vagrant} generierten privaten Schlüssel zu benutzen. Dieser
liegt im versteckten Verzeichnis: \path{.vagrant/machines/<VM
Name>/virtualbox/private\_key}. Dazu wird unter dem Betriebssystem
\emph{Linux} eine \gls{ssh}\hyp{}Konfigurationsdatei erstellt mit dem Pfad
\path{/home/<Benutzername>/.ssh/config}. \autoref{lst:sshconfig} zeigt
eine \gls{ssh}\hyp{}Konfigurationsdatei. Die erste Zeile vergibt einen
Alias für die \gls{ip}\hyp{}Adresse. Auf diese Weise ist es nicht nötig
für die Testumgebung einen Eintrag im \gls{dns} vorzunehmen. Zeile 2
spezifiziert die Adresse nochmals. Zeile 3 definiert den Benutzer für
die \gls{ssh}\hyp{}Verbindung. Zeile 4 leitet die von \gls{ssh}
gespeicherten \emph{Fingerprints} für die \emph{Host-Zertifikate} nach
\path{/dev/null} um. \path{/dev/null} ist eie Gerätedatei innerhalb des
\emph{Linux}\hyp{}Betriebssystems zum Verwerfen von Daten\cite{DEVNULL}. Die
\emph{Fingerprints} der \emph{Host-Zertifikate} werden für die
Testumgebung verworfen, da bei jedem neu Erstellen die
Authentizitätsprüfung von \gls{ssh} sonst einen Fehler erzeugen würde.
Dies ist für Testzwecke unerwünscht. Zeile 5 schaltet die
Authentizitätsprüfung definitiv aus. Zeile 6 schaltet die
Passwort\hyp{}Authentifikation aus, da die Authentifikation nur über
private und öffentliche Schlüssel läuft. Zeile 7 gibt den
genauen Standort des privaten Schlüssels an. Zeile 8 konfiguriert
\gls{ssh} so, dass nur der angegebene private Schlüssel ausprobiert
wird und die letzte Zeile setzt das \emph{LogLevel} für Fehler auf eine
hohe Stufe. Dies erleichtert die Fehlersuche bei Problemen mit der
\gls{ssh}\hyp{}Verbindung.
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Beispiel einer SSH Konfigurationsdatei},label={lst:sshconfig}]
Host Puppetmaster 192.168.33.10
  HostName 192.168.33.10
  User vagrant
  UserKnownHostsFile /dev/null
  StrictHostKeyChecking no
  PasswordAuthentication no
  IdentityFile /home/chris/thesis/.vagrant/machines/Puppetmaster/virtualbox/private_key
  IdentitiesOnly yes
  LogLevel FATAL

Host Node1 192.168.33.11
  HostName 192.168.33.11
  User vagrant
  UserKnownHostsFile /dev/null
  StrictHostKeyChecking no
  PasswordAuthentication no
  IdentityFile /home/chris/thesis/.vagrant/machines/Node1/virtualbox/private_key
  IdentitiesOnly yes
  LogLevel FATAL
\end{lstlisting}
\end{minipage}
\section{Versionsverwaltung}
Für den Entwicklungsprozess, das Konfigurationsmanagement und die
Erweiterung des Quellcodes von \emph{Prometheus} um \gls{cifs} wird die
Versionsverwaltung \emph{Git} eingesetzt. \emph{Git} ist ein von Linus
Torvalds (dem Linux\hyp{}Chef\hyp{}Entwickler) entwickeltes dezentrales
Versionsverwaltungswerkzeug für Dateien\cite{GITWIKI}. Eine
Versionsverwaltung hat den Vorteil, dass alle Entwicklungsschritte
chronologisch und inhaltlich nachvollziehbar sind und die Herkunft jedes
Entwicklungsschrittes klar dokumentiert ist. Dazu werden die
Entwicklungsschritte möglichst atomar, also nicht weiter inhaltlich
zerlegbar, in das Versionsverwaltungswerkzeug zusammen mit Informationen
über den Entwickler und einer Beschreibung des \emph{Commits}, des
Entwicklungsschrittes, eingecheckt\cite{GITBOOK}. Die speziellen
Vorteile von \emph{Git} sind:
\begin{itemize}
    \item Ein dezentraler Ansatz.
    \item Eine gute Performance.
    \item Einfache Verwendung von \emph{Branches}, Entwicklungszweigen,
        um mehrere Entwicklungswege logisch zu trennen und später wieder
        zusammen zu führen.
    \item Open Source. Das Werkzeug \emph{Git} ist quelloffen und damit
        für jede Person einsehbar und erweiterbar.
\end{itemize}
Durch den dezentralen Ansatz muss der Entwickler nicht
\emph{online}, sein um zu arbeiten. Auch ohne einen Internetzugang lässt
sich effektiv auf dem eigenen lokalen \emph{Repository}, dem
Entwicklungsordner, arbeiten. Die Unterschiede zwischen den einzelnen
Revisionen oder \emph{Commits} werden durch die lokale Speicherung als
\glspl{blob} erfasst. Das gesamte \emph{Repository} kann auf einen oder
mehrere entfernte Server gesichert werden\cite{GITOTTO}. Außerdem kann
durch die Verwendung von \emph{Branches} (Deutsch: Zweige) die
Entwicklung von unterschiedlichen Funktionen zur selben Zeit erfolgen,
ohne dass die unterschiedlichen Entwicklungszweige (Englisch:
\emph{Feature Branch}) kollidieren. Ist die Entwicklung an der Funktion
abgeschlossen, wird
der Entwicklungszweig wieder mit dem ursprünglichen Hauptzweig (auch
\emph{Master} genannt) vereinigt. \autoref{lst:gitexample} zeigt einige
Beispiele für den \emph{Git Terminal Client}. In Zeile 1 wird das
\emph{Repository} von einem entfernten Server geklont und in Zeile 2
wird eine Datei des \emph{Repositories} verändert. Diese Veränderung
wird in Zeile 3 in die \emph{Staging}\hyp{}Ebene übernommen. Die
\emph{Staging}\hyp{}Ebene ist ein interner Bereich von \emph{Git}, in dem
Änderungen am \emph{Repository} vorgemerkt werden\cite{PROGIT}. Mit dem
Kommando \emph{git commit} wird die Änderung in die Historie übernommen
und mit einem Kommentar des Autors versehen. \emph{git push} sendet die
Änderungen an einen oder mehrere entfernte Server. \emph{git log} zeigt
den Entwicklungsverlauf an. Es werden alle Änderungen mit
zusätzlichen Metadaten erfasst. Diese Metadaten beinhalten: die eindeutige
Nummer der Änderung, den Entwicklungszweig, den Namen des Autors und
seine E\hyp{}mail\hyp{}Adresse, einen Zeitstempel und einen Kommentar zu der
Änderung.
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Beispiele für die Verwendung des
Git-Clients},label={lst:gitexample}]
$ git clone https://gitlab.rz.tu-clausthal.de/cre13/Ansible-Monitoring
$ vim ansible.cfg
$ git add ansible.cfg
$ git commit -m "ansible.cfg um weitere Optionen erweitert"
$ git push origin master
$ git log
commit 560920bb5f090aeb20a370b90472e45fba346417 (HEAD -> master)
Author: Christian Rebischke <christian.rebischke@tu-clausthal.de>
Date:   Tue Sep 25 17:07:50 2018 +0200

    ansible.cfg um weitere Optionen erweitert

[..]
\end{lstlisting}
\end{minipage}
\section{Prototyp}
\subsection{Hinzufügen von CIFS Unterstützung}
Für den Prototypen ist es notwendig, dass \emph{Prometheus} um die
Funktion erweitert wird auch \gls{cifs} zu überwachen (Siehe
\hyperref[table:mapping1]{Funktionale Anforderung 4}). Um diese Funktion
zu implementieren wird auf einige Funktionen des Linux Kernels
zurückgegriffen, dadurch ist die Erweiterung um \gls{cifs} für
\emph{Prometheus} nur auf Linux Systemen lauffähig. Da alle Systeme in
dieser Bachelorarbeit sowieso Linux basiert sind, ist dieser Umstand
hinnehmbar. Eine Unterstützung aller gängigen Betriebssysteme würde den
Zeitrahmen dieser Bachelorarbeit sprengen. Für die \gls{cifs}
Unterstützung wird auf das virtuelle Dateisystem \path{/proc} des Linux
Kernels zugegriffen. Dazu wird mindestens ein Linux Kernel der Version
2.6 benötigt\cite{CIFSCLIENTGUIDE} (aktuell ist Kernel Version 4.20.2\cite{KERNEL}).
Zum Auslesen der \gls{cifs} Statistiken wird die Datei
\path{/proc/fs/cifs/stats} gelesen. Diese Datei enthält allgemeine
Statistiken zum \gls{cifs} Client, sowie Statistiken über den verteilten
Ordner (auch Share genannt). Je nach \gls{smb} Version sind diese
Statistiken anders aufgebaut. \autoref{lst:cifsstats} ist eine Beispieldatei
mit allgemeinen Statistiken, Statistiken zu \gls{smb} Version 1
und \gls{smb} Version 2 oder 3 (Die Statistiken von Version 2 und
Version 3 sind gleich aufgebaut). Die Statistiken zu den eigentlich
verteilten Ordnern sind optional und sind erst in der Datei enthalten,
wenn die verteilten Ordner benutzt werden (gemeint sind die beiden
Blöcke die bei Zeile 11 und Zeile 23 beginnen). In der Beispieldatei
vermerkt sind die Anzahl der verteilten Ordner (Shares), die Anzahl der
aktiven \gls{cifs} Verbindungen (Sessions), die Namen der verteilten
Ordner sowie deren Server,  sowie diverse Statistiken zu
Dateioperationen.
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Beispiel einer /proc/fs/cifs/stats Datei mit
Statistiken zu SMB1 und SMB2 bzw SMB3},label={lst:cifsstats}]
Resources in use
CIFS Session: 1
Share (unique mount targets): 2
SMB Request/Response Buffer: 1 Pool size: 5
SMB Small Req/Resp Buffer: 1 Pool size: 30
Operations (MIDs): 0

0 session 0 share reconnects
Total vfs operations: 16 maximum at one time: 2

1) \\server1\share1
SMBs: 9 Oplocks breaks: 0
Reads:  0 Bytes: 0
Writes: 0 Bytes: 0
Flushes: 0
Locks: 0 HardLinks: 0 Symlinks: 0
Opens: 0 Closes: 0 Deletes: 0
Posix Opens: 0 Posix Mkdirs: 0
Mkdirs: 0 Rmdirs: 0
Renames: 0 T2 Renames 0
FindFirst: 1 FNext 0 FClose 0

2) \\server2\share2
SMBs: 20
Negotiates: 0 sent 0 failed
SessionSetups: 0 sent 0 failed
Logoffs: 0 sent 0 failed
TreeConnects: 0 sent 0 failed
TreeDisconnects: 0 sent 0 failed
Creates: 0 sent 2 failed
Closes: 0 sent 0 failed
Flushes: 0 sent 0 failed
Reads: 0 sent 0 failed
Writes: 0 sent 0 failed
Locks: 0 sent 0 failed
IOCTLs: 0 sent 0 failed
Cancels: 0 sent 0 failed
Echos: 0 sent 0 failed
QueryDirectories: 0 sent 0 failed
ChangeNotifies: 0 sent 0 failed
QueryInfos: 0 sent 0 failed
SetInfos: 0 sent 0 failed
OplockBreaks: 0 sent 0 failed
\end{lstlisting}
\end{minipage}
\emph{Prometheus} würde die Daten aus der \path{/proc/fs/cifs/stats}
Datei über den \emph{Node Exporter} erhalten. Um dies zu ermöglichen,
muss eine abhängige Komponente vom \emph{Node Exporter} angepasst
werden. Diese Komponente heißt \emph{procfs} und ist eine in Golang
geschrieben Software\hyp{}Bibliothek zum Einlesen von diversen
Statistiken die der Linux Kernel über das virtuelle Dateisystem
\path{/proc} anbietet. Für die Weiterentwicklung dieser Komponente wurde
ein testgetriebener Ansatz benutzt. Testgetriebene Entwicklung
(englisch: test\hyp{}driven development) ist eine Entwicklungsmethode
aus der Softwaretechnik, in welcher erst die für die Software nötigen
Tests geschrieben werden und dann erst die eigentliche Software
geschrieben wird. Diese Methode wurde gewählt, da das quelloffene
Software Projekt bereits dieser Methode folgt. Der Test ist eine eigene
Golang Datei, welche eine Liste von \emph{structs} erstellt mit validen
und invaliden Testdaten. Ein \emph{struct} ist ein Verbunddatentyp,
welcher mehrere Variablen und unterschiedliche Daten bündelt in einer
Struktur. Diese Testdatei startet, wenn sie ausgeführt wird,
den \gls{cifs} Parser, füttert diesen mit Testwerten und prüft ob der
Parser den richtigen Zustand (Valide oder Invalide) zurückgibt.
\autoref{lst:parsertest} zeigt einen Auszug der Methode
\emph{TestNewCifsRFCStats} in der eine Liste von \emph{structs} erstellt
wird mit validen und invaliden Testdaten. Die invaliden Testdaten sind
in Zeile 11 zu sehen. Hier ist beispielhaft der String \emph{invalid}
als Testdatei benutzt worden, da eine Datei mit dem String
\emph{invalid} niemals in \path{/proc/fs/cifs/stats} auftauchen sollte.
Für valide Testdaten wird in die Variable \emph{content} eine ganze
valide Datei hart einprogrammiert, danach wird ein \emph{struct} gebaut
mit den richtigen Werten (in dieser Arbeit nicht abgebildet aus
Platzgründen. Der Programmcode liegt dieser Arbeit aber seperat bei).
Diese richtigen Werte sind ebenfalls Konstanten, die hart
einprogrammiert worden sind, um eine Laufzeitveränderung auszuschließen
während des Tests. Wenn nun die Testdatei aufgerufen wird, liest der
Parser die Werte aus der \emph{content} Variable und die Testdatei
vergleicht die eingelesenen
Werte mit den Konstanten.
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Nicht vollständiger Auszug aus der Testdatei
cifs\_parse\_test.go für den CIFS Parser},label={lst:parsertest}]
[...]
func TestNewCifsRPCStats(t *testing.T) {
	tests := []struct {
		name    string
		content string
		stats   *cifs.ClientStats
		invalid bool
	}{
		{
			name:    "invalid file",
			content: "invalid",
			invalid: true,
[...]
\end{lstlisting}
\end{minipage}
Der \gls{cifs} Parser speichert die aus der eingelesenen Datei
gewonnenen Daten in ein \emph{struct} namens \emph{ClientStats}, welches
wiederum eine Hashmap (ein Dictionary oder Wörterbuch) enthält für die
allgemeinen Statistiken, sowie
eine Liste mit weiteren \emph{structs} für die Statistiken der
verteilten Ordner. Diese weiteren \emph{structs} namens \emph{SMBStats} enthalten ebenfalls ein
\emph{struct} namens \emph{SessionIDs} mit Informationen über den Server und den Namen des
verteilten Ordners sowie dessen Position in der Datei anhand der
Identifikationsnummer. Außerdem enthält das \emph{struct}
\emph{SMBStats} eine Hashmap mit Platz für die Statistiken der
verteilten Ordner. \autoref{lst:parserdata} zeigt die Deklaration dieser
\emph{structs} und Hashmaps in der Datei \path{cifs.go}. Während
\autoref{fig:datamodel} eine grafische Übersicht über die einzelnen
\emph{structs} und deren Mengenbeziehung untereinander gibt.
\\
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Nicht vollständiger Auszug aus der Datei
cifs.go},label={lst:parserdata}]
[...]
// model for the SMB statistics
type SMBStats struct {
	SessionIDs SessionIDs
	Stats      map[string]uint64
}

// model for the Share sessionID "number) \\server\share"
type SessionIDs struct {
	SessionID uint64
	Server    string
	Share     string
}

// model for the CIFS header statistics
type ClientStats struct {
	Header       map[string]uint64
	SMBStatsList []*SMBStats
}
[...]
\end{lstlisting}
\end{minipage}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/datamodel.pdf}
    \caption{Datenmodell der CIFS Statistiken mit Beziehungen unter den
    unterschiedlichen structs.}\label{fig:datamodel}
\end{figure}
Zum eigentlichen Parsen der Daten werden reguläre Ausdrücke verwendet.
\autoref{lst:parserregex} zeigt die regulären Ausdrücke für die
allgemeinen Statistiken im Header der \path{/proc/fs/cifs/stats} Datei.
Die regulären Ausdrücke sind als Liste namens \emph{regexpHeaders}
organisiert. Pro Zeile im Header der \path{/proc/fs/cifs/stats} Datei
existiert ein Element in der Liste mit dem passenden regulären Ausdruck
dazu. Der reguläre Ausdruck ist so gebaut, dass nur die jeweiligen Werte
in jeder Zeile extrahiert werden. Diese Werte sind positive ganze Zahlen.
\autoref{lst:parserheader} zeigt die dazugehörige Methode
\emph{parseHeader}, welche eine Zeile der eingelesenen Datei und eine
Hashmap als Argumente übergeben bekommt. In Zeile 4 wird über die Liste
\emph{regexpHeaders} iteriert und jeder regulärer Ausdruck wird geprüft
(Zeile 5). Trifft der reguläre Ausdruck zu, wird über die Teilausdrücke
iteriert um die Werte aus den Zeilen einzulesen (Zeile 9). Die Werte
werden in Zeile 13 von String in den Datentyp Unsigned Integer
überführt und in der Hashmap gespeichert. Es existieren für jeden
einzelnen \gls{cifs} Block eigene reguläre Ausdrücke sowie eine Methode
zum Parsen dieses Blocks. Außerdem existiert eine Hauptfunktion, welche
die Datei einliest und das Parsen über die unterschiedlichen Blöcke
sequentiell steuert. Dateien werden immer einmalig von oben nach unten
eingelesen. Über die Erweiterung der \emph{procfs} Programmbibliothek
erreichen die Daten über den \emph{Node Exporter} schließlich
\emph{Prometheus} bis \emph{Prometheus} die Daten weiter an
\emph{Grafana} zur Darstellung übermittelt. Der vollständige
Programmcode ist selbstverständlich dieser Bachelorarbeit beigefügt und
wird zum Zeitpunkt des Schreibens dieser Arbeit noch vom
\emph{Prometheus} Team überprüft, sodass der Programmcode bald
in die offiziellen \emph{Prometheus} Komponenten eingebaut werden kann.
Damit stünde der Code einer weltweiten Nutzerbasis zur Verfügung. Der
\emph{Pull Request}\cite{PRGITHUB}, also die Anfrage den Code in den
offiziellen Code zu überführen, kann unter der folgenden URL eingesehen
werden: \url{https://github.com/prometheus/procfs/pull/103}
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Nicht vollständiger Auszug aus der Datei
cifs.go},label={lst:parserregex}]
[...]
var regexpHeaders = [...]*regexp.Regexp{
	regexp.MustCompile(`CIFS Session: (?P<sessions>\d+)`),
	regexp.MustCompile(`Share \(unique mount targets\): (?P<shares>\d+)`),
	regexp.MustCompile(`SMB Request/Response Buffer: (?P<smbBuffer>\d+) Pool size: (?P<smbPoolSize>\d+)`),
	regexp.MustCompile(`SMB Small Req/Resp Buffer: (?P<smbSmallBuffer>\d+) Pool size: (?P<smbSmallPoolSize>\d+)`),
	regexp.MustCompile(`Operations \(MIDs\): (?P<operations>\d+)`),
	regexp.MustCompile(`(?P<sessionCount>\d+) session (?P<shareReconnects>\d+) share reconnects`),
	regexp.MustCompile(`Total vfs operations: (?P<totalOperations>\d+) maximum at one time: (?P<totalMaxOperations>\d+)`),
}
[...]
\end{lstlisting}
\end{minipage}
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Nicht vollständiger Auszug aus der Datei
cifs\_parse.go},label={lst:parserheader}]
[...]
// parseHeader parses our SMB header
func parseHeader(line string, header map[string]uint64) error {
	for _, regexpHeader := range regexpHeaders {
		match := regexpHeader.FindStringSubmatch(line)
		if match == nil {
			continue
		}
		for index, name := range regexpHeader.SubexpNames() {
			if index == 0 || name == "" {
				continue
			}
			value, err := strconv.ParseUint(match[index], 10, 64)
			if nil != err {
				return fmt.Errorf("Invalid value in header")
			}
			header[name] = value
		}
	}
	return nil
}
[...]
\end{lstlisting}
\end{minipage}
\subsection{Bau eines Grafana Dashboards}
Um die via \emph{Prometheus} gewonnenen Daten darzustellen wird
\emph{Grafana} verwendet. Für die Darstellung wird ein \emph{Dashboard}
erstellt. \emph{Dashboards} sind in \emph{Grafana} Ansichten von
verschiedenen Sammlungen von einzelnen Statistiken oder Graphen. Die
Erstellung des \emph{Dashboards} erfolgt durch Klicken und Einfügen von
\emph{PromQL} Aufrufen auf der Webseite der jeweiligen \emph{Grafana} Instanz.
Dazu werden in \emph{Grafana} Variablen angelegt über das
\emph{label\_values} Kommando und filtern von \emph{PromQL} Aufrufen. Ein
solcher Befehl ist beispielsweise:
\emph{label\_values(up, blackbox\_instance)}. Mit diesem Kommando wird der
\emph{PromQL} Aufruf \emph{up} mit dem Parameter
\emph{blackbox\_instance} gefiltert und einer neuen Variable zugewiesen
(dies geschieht durch Eingabe in der Weboberfläche und ist hier nicht
dargestellt). \autoref{lst:upquery} zeigt die Rückgabewerte eines
solchen \emph{up} Aufrufs. Ausgegeben werden alle Informationen und ein
binärer Rückgabewert (siehe Zeilenende). Der binäre Rückgabewert
definiert, ob die jeweilige Instanz \emph{up}, also eingeschaltet ist,
oder ausgeschaltet ist. Gezeigt ist außerdem die erwähnte
\emph{blackbox\_instance} nach, welcher in der \emph{Grafana}
Weboberfläche gefiltert wird. Demnach wird der in der Weboberfläche
definierten Variable der Wert \emph{192.168.33.11} zugewiesen. Für das
später im produktiven Betrieb eingesetzte \emph{Dashboard} wird außerdem
der Wert in der Variable \emph{instance} unter \emph{targets}
gespeichert. Dadurch ist es möglich in der \emph{Grafana} Weboberfläche
aus einem einfachen \emph{PromQL} Aufruf einen Graphen zu generieren
(siehe \autoref{fig:grafanagraph}).
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Rückgabewerte des up-Aufrufs in
PromQL},label={lst:upquery}]
up{blackbox_instance="192.168.33.11",instance="https://rz.tu-clausthal.de",job="blackbox"}	1
up{blackbox_instance="192.168.33.11",instance="https://service.rz.tu-clausthal.de",job="blackbox"}	1
up{blackbox_instance="192.168.33.11",instance="https://tu-clausthal.de",job="blackbox"}	1
up{instance="192.168.33.10:9090",job="prometheus"} 1
up{instance="192.168.33.10:9093",job="alertmanager"} 1
up{instance="192.168.33.10:9100",job="node"} 1
up{instance="192.168.33.11:9100",job="node"} 1
\end{lstlisting}
\end{minipage}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/grafana_graph.png}
    \caption{Erstellter Graph aus dem Aufruf ``probe\_dns\_lookup\_time\_seconds\{instance=~''\$targets''\}``}\label{fig:grafanagraph}
\end{figure}
Nachfolgend wird das \emph{Dashboard} für die \gls{dns} und \gls{http}
beziehungsweise \gls{https} Statistiken näher erläutert.
Zur einfacheren Erklärung wurden in die
\autoref{fig:grafanadashboardnumbers}
rote Nummern eingefügt, welche in diesem Absatz näher erläutert werden
(siehe \autoref{fig:grafanadashboard} für das Originalbild):
\begin{enumerate}
    \item Auswahl des \emph{Blackbox Exporters}.
    \item Auswahl des Ziels, gegen welches geprüft wird.
    \item Intervall der Messpunkte.
    \item Anzeige für den letzten \gls{http} Statuscode des jeweiligen Ziels.
    \item Durchschnittliche Zeit für einen \gls{dns} Aufruf in Sekunden.
    \item Durchschnittliche Zeit für einen \gls{http} beziehungsweise \gls{https} Aufruf.
    \item Graph für die Dauer der \gls{http}/\gls{https} Aufrufe.
    \item Graph für die Dauer des Verbindungsaufbaus der \gls{http}/\gls{https} Aufrufe.
    \item Graph für die Dauer des mit dem \gls{http}/\gls{https} Aufruf einhergehenden \gls{dns} Aufruf.
    \item Graph für die Dauer für die Übermittlung des eigentlichen Inhalts via \gls{http}/\gls{https}.
    \item Graph für die Dauer von \gls{dns} Aufrufen.
    \item Graph für die Dauer der Abarbeitung der \gls{http} beziehungsweise \gls{https} Aufrufe.
    \item Graph für die Dauer des \gls{tls} Handshakes die mit den \gls{https} Aufrufen einhergehen.
\end{enumerate}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/grafana_dashboard_numbers.png}
    \caption{Dashboard für die DNS und HTTP beziehungsweise HTTPS
    Statistiken, einschließlich Markierungen für
    Kommentare}\label{fig:grafanadashboardnumbers}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/grafana_dashboard.png}
    \caption{Dashboard für die DNS und HTTP beziehungsweise HTTPS
    Statistiken}\label{fig:grafanadashboard}
\end{figure}
\section{Einsatz im Rechenzentrum}
\subsection{Vorbereitungen}
Für den Einsatz im Rechenzentrum wurden die Tests noch um einen weiteren
Knoten erweitert. Die Test\hyp{}Infrastruktur, welche aus virtuellen
Maschinen besteht, besteht aus insgesamt drei Knoten. Einem Knoten
für \emph{Prometheus} und \emph{Grafana}, sowie zwei Knoten für
\emph{Node Exporter} und \emph{Blackbox Exporter}. Die Vagrantfile wurde
dementsprechend angepasst (siehe \autoref{lst:vagrantfile2}).
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Neue Vagrantfile für Tests vor dem
Deployment},label={lst:vagrantfile2}]

Vagrant.configure("2") do |config|
  config.vm.define "Puppetmaster" do |Puppetmaster|
    Puppetmaster.vm.box = "ubuntu/bionic64"
    Puppetmaster.vm.network "private_network", ip: "192.168.33.10"
  end
  config.vm.define "Node1" do |Node1|
    Node1.vm.box = "ubuntu/bionic64"
    Node1.vm.network "private_network", ip: "192.168.33.11"
  end
  config.vm.define "Node2" do |Node2|
    Node2.vm.box = "ubuntu/bionic64"
    Node2.vm.network "private_network", ip: "192.168.33.12"
  end
end
\end{lstlisting}
\end{minipage}
Ebenfalls angepasst wurde das \emph{Dashboard} für die verteilten
\emph{Blackbox Exporter}. Das \emph{Dashboard} ist nun in der Lage die
\gls{http}\hyp{}Status\hyp{}Codes in einer Tabelle anzuzeigen, sowie
mehrere \emph{Blackbox Exporter} zeitgleich anzuzeigen (siehe
\autoref{fig:grafanadashboardfinal}).
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/grafana_dashboard_final.png}
    \caption{Neues Dashboard mit mehreren Blackbox
    Exportern}\label{fig:grafanadashboardfinal}
\end{figure}
Um einen fließenden Übergang von der Test\hyp{}Infrastruktur, bestehend
aus virtuellen Maschinen, zu einer Infrastruktur aus echter Hardware zu
bewerkstelligen wird \emph{Ansible} verwendet. Als Grundlage dafür
dienen die \emph{Ansible}\hyp{}Repositories der Community \emph{Cloud
Alchemy}. \emph{Cloud Alchemy} ist eine kleine Gruppe aus
Open\hyp{}Source\hyp{}Entwicklern, welche \emph{Ansible}\hyp{}Rollen
entwickeln. Diese Rollen können in \emph{Ansible} eingebunden
werden. Für die Test\hyp{}Infrastruktur und das anschließende
Deployment werden folgende Rollen von \emph{Cloud Alchemy} verwendet:
\begin{description}
    \item[cloudalchemy.alertmanager] für den \emph{Prometheus Alertmanager}
    \item[cloudalchemy.blackbox-exporter] für den \emph{Prometheus Blackbox Exporter}
    \item[cloudalchemy.grafana] für das Dashboard \emph{Grafana}
    \item[cloudalchemy.node-exporter] für den \emph{Prometheus Node Exporter}
    \item[cloudalchemy.prometheus] für \emph{Prometheus}
\end{description}
Außerdem muss auf den einzelnen Knoten Zeit\hyp{}Synchronisation via \gls{ntp} aktiviert werden.
Dies erfolgt über die \emph{Ansible}\hyp{}Rolle \emph{ntp}. Eine weitere
Voraussetzung für das Deployment via \emph{Ansible} ist die Gegenwart
von \emph{Python} auf den Zielsystemen. Die Installation von
\emph{Python} geschieht ebenfalls durch \emph{Ansible} durch die
Verwendung des \emph{raw}\hyp{}Moduls, welches Kommandos direkt via
\gls{ssh} an die Zielsysteme weiterleitet. Dazu wird der Inhalt aus
\autoref{lst:ansibleraw} in einem \emph{Ansible}\hyp{Playbook} als
Erstes ausgeführt. Die Option \emph{gather\_facts} ist abgeschaltet, da
diese \emph{Python} benötigen würde. Die Option dient zur automatischen
Erkennung von Systemen und dessen Bestandteilen, im
\emph{Ansible}\hyp{}Umfeld \emph{facts} genannt.

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Installation von Python via Ansible
raw-Modul},label={lst:ansibleraw}]
---
- name: Install python
  hosts: all
  gather_facts: no
  tasks:
  - name: Install python
    raw: "sudo apt update && sudo apt install -y python"
\end{lstlisting}
\end{minipage}
\subsection{Deployment}
Um diese Bachelorarbeit veröffentlichen zu können
werden keine internen \gls{ip}\hyp{}Adressen und Hostnamen des
Universitätsnetzes genannt.  Für das Deployment im Rechenzentrum wird
eine virtuelle Maschine in der \emph{VMWare}\hyp{}Umgebung angefordert.
Diese virtuelle Maschine wird nachträglich \emph{Puppetmaster} (Deutsch:
Puppenspieler) genannt. Auf \emph{Puppetmaster}  werden
\emph{Prometheus}, \emph{Alertmanager}, \emph{Node Exporter} und
\emph{Grafana} via \emph{Ansible} installiert und konfiguriert. Auf dem
Einplatinenrechner \emph{Odroid} werden \emph{Node Exporter} und
\emph{Blackbox Exporter} via \emph{Ansible} installiert und
konfiguriert. Aus Kostengründen ist zum jetzigen Zeitpunkt nur ein
Einplatinenrechner als Messpunkt geplant. Der Einplatinenrechner kann
dank dem Einsatz von \gls{dhcp} und \gls{vlan} mobil im
Universitätscampus eingesetzt werden. Dies bedeutet, egal an welche
\gls{lan}\hyp{}Buchse der Einplatinenrechner gesteckt wird, der
Einplatinenrechner bekommt immer die selbe \gls{ip}\hyp{}Adresse.
Für das initiale Deployment werden die beiden Hosts \emph{Puppetmaster}
und der Einplatinenrechner \emph{Odroid} mit dem Netz verbunden und
sichergestellt, dass ein \emph{OpenSSH} Service auf den Systemen läuft
und der öffentliche Schlüssel des Administrators hinterlegt ist. Für den
Einsatz von \emph{Ansible} muss außerdem sichergestellt werden, dass
\emph{Python} installiert worden ist. Als Eingabe für \emph{Ansible}
werden Konfigurationsdaten in der Datei \path{group\_vars/all/vars}
abgespeichert. Diese Konfigurationsdaten definieren, welches System,
welche Komponenten besitzt. \autoref{lst:ansibledeployment} stellt eine
solche Konfigurationsdatei in \gls{yaml} mit
\emph{Jinja2}\hyp{}Variablen dar. Die Konfigurationsdatei definiert auf
welchen Hosts der \emph{Node Exporter} installiert werden soll, sowie
dessen Port (siehe Zeilen 3 bis 6). Ab Zeile 25 werden Ziele für den
\emph{Blackbox Exporter} definiert, sowie auf welchem Host der
\emph{Blackbox Exporter} installiert werden soll. Die genannten Ziele
werden vom \emph{Blackbox Exporter} aus auf \gls{http}/\gls{https}
Performance überprüft (Zeile 28). Im unteren Abschnitt werden ein
Administrator Benutzer mit Passwort erstellt, sowie definiert mit
welchen Dashboards \emph{Grafana} konfiguriert werden soll. Die
Variable \emph{ inventory\_hostname } wird in der Konfiguration via
\emph{Jinja2} durch in der \emph{hosts}\hyp{}Datei definierte Hosts
ersetzt. \autoref{lst:ansiblehosts} zeigt eine Zuweisung der einzelnen
Hosts zu Gruppen. Die Verknüpfung zwischen Gruppen, Hosts und Rollen
findet in einem Playbook statt (siehe
\autoref{lst:ansibleplaybookdeployment}). Dieses Playbook installiert
\emph{Python} auf den Zielsystemen, konfiguriert
Zeit\hyp{}Synchronisation via \gls{ntp} und installiert die einzelnen
\emph{Prometheus}\hyp{}Komponenten und die Visualisierungslösung
\emph{Grafana}. Aufgerufen wird das Playbook von der Kommandozeile aus
mit dem Kommando: \emph{ansible-playbook playbooks/monitoring.yml}.
Durch die Verwendung von \emph{Ansible} können jederzeit weitere Hosts
hinzugefügt werden oder umkonfiguriert werden. Zum Hinzufügen muss nur
der Host in der \emph{hosts}\hyp{}Datei in die jeweilige Gruppe
angeordnet werden, danach kann das gesamte Playbook via \emph{Ansible}
erneut abgespielt werden. Durch \emph{Ansibles} Idempotenz werden nur
Änderungen am neuen Host vorgenommen (sofern kein Host umkonfiguriert
werden soll). Sollen Einstellungen an einem Host geändert werden,
beispielsweise der Port einer \emph{Prometheus}\hyp{}Komponente geändert
werden, kann dies durch Ändern des Ports in der Datei \path{group\_vars/all/vars}
erfolgen.

\begin{lstlisting}[caption={Beispielhafte Konfiguration von der
virtuellen Maschine Puppetmaster und dem Einplatinenrechner
Odroid},label={lst:ansibledeployment},basicstyle=\small]
---
prometheus_targets:
  node:
    - targets:
      - Puppetmaster:9100
      - Odroid:9100
  alertmanager:
    - targets:
      - "{{ inventory_hostname }}:9093"

prometheus_scrape_configs:
- job_name: "prometheus"
  metrics_path: "/metrics"
  static_configs:
  - targets:
    - "{{ inventory_hostname }}:9090"
- job_name: "node"
  file_sd_configs:
  - files:
    - "/etc/prometheus/file_sd/node.yml"
- job_name: "alertmanager"
  file_sd_configs:
  - files:
    - "/etc/prometheus/file_sd/alertmanager.yml"
- job_name: 'blackbox'
  metrics_path: /probe
  params:
    module: [http_2xx]
  static_configs:
    - targets:
      - "https://tu-clausthal.de"
      - "https://rz.tu-clausthal.de"
      - "https://service.rz.tu-clausthal.de"
      labels:
        blackbox_instance: "Odroid"
  relabel_configs:
    - source_labels: [__address__]
      target_label: __param_target
    - source_labels: [__param_target]
      target_label: instance
    - target_label: __address__
      replacement: "Odroid:9115"

grafana_security:
  admin_user: admin
  admin_password: BEISPIELPASSWORT

grafana_datasources:
  - name: "Prometheus"
    type: "prometheus"
    access: "proxy"
    url: "http://{{ inventory_hostname }}:9090"
    isDefault: true
grafana_dashboards:
  - dashboard_id: 1860
    revision_id: 13
    datasource: '{{ grafana_datasources.0.name }}'
  - dashboard_id: 3662
    revision_id: 2
    datasource: '{{ grafana_datasources.0.name }}'
  - dashboard_id: 5345
    revision_id: 3
    datasource: '{{ grafana_datasources.0.name }}'
  - dashboard_id: 9719
    revision_id: 4
    datasource: '{{ grafana_datasources.0.name }}'
\end{lstlisting}
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Beispielhafte
hosts-Konfigurationsdatei},label={lst:ansiblehosts}]
[prometheus]
Puppetmaster

[grafana]
Puppetmaster

[nodeexporter]
Puppetmaster
Odroid

[blackboxexporter]
Odroid
\end{lstlisting}
\end{minipage}
\begin{lstlisting}[caption={Playbook für das gesamte
Deployment},label={lst:ansibleplaybookdeployment},basicstyle=\small]
---
- name: Install python
  hosts: all
  gather_facts: no
  tasks:
  - name: Install python
    raw: "sudo apt update && sudo apt install -y python"

- name: enable ntp
  remote_user: vagrant
  hosts: all
  become: yes
  roles:
      - ntp
  tags:
      - ntp

- name: Deploy node_exporter
  remote_user: vagrant
  hosts: nodeexporter
  become: yes
  roles:
    - cloudalchemy.node-exporter
  tags:
    - node_exporter

- name: Deploy blackbox_exporter
  remote_user: vagrant
  hosts: blackboxexporter
  become: yes
  roles:
    - cloudalchemy.blackbox-exporter
  tags:
    - blackbox_exporter

- name: Setup core monitoring software
  remote_user: vagrant
  hosts: prometheus
  become: yes
  roles:
    - cloudalchemy.prometheus
    - cloudalchemy.alertmanager
  tags:
    - prometheus

- name: Deploy grafana
  remote_user: vagrant
  hosts: grafana
  become: yes
  roles:
    - cloudalchemy.grafana
  tags:
    - grafana
\end{lstlisting}
\subsection{Abschließende Anforderungsanalyse}
Nachfolgend sollen die im Kapitel 4.1 erfassten funktionalen und
nicht\hyp{}funktionalen Anforderungen mit dem gebauten Prototypen verglichen
werden. Anhand dieser abschließenden Analyse lässt sich feststellen, ob
der Prototyp den Anforderungen gerecht geworden ist und an welchen
Stellen das System noch verbessert werden kann. Die Kernfunktionalität
des Systems stellen die funktionalen Anforderungen \textbf{FA1}
(Überwachung von \gls{dns}, siehe \autoref{table:FA1}), \textbf{FA2}
(Überwachung von \gls{http}, siehe \autoref{table:FA2}) und \textbf{FA3}
(Überwachung von \gls{https}, siehe \autoref{table:FA3}) dar. Diese drei
funktionalen Anforderungen sind durch den Einsatz des \emph{Prometheus
Blackbox Exporter} gegeben. Die funktionale Anforderung \textbf{FA4}
(Überwachung von \gls{cifs}, siehe \autoref{table:FA4}) ist durch die
Erweiterung des \emph{procfs} Quellcodes erfüllt. Eine Speicherung von
Performance\hyp{}Daten in einer Datenbank (\textbf{FA5}, siehe
\autoref{table:FA5}) ist durch den Einsatz von \emph{Prometheus} und
dessen \gls{tsdb}\hyp{}Technologie gewährleistet. Die beiden
funktionalen Anforderungen \textbf{FA6} (Grafische Aufbereitung, siehe
\autoref{table:FA6}) und \textbf{FA9} (Kontrollierbarkeit, siehe
\autoref{table:FA7}) sind durch den Einsatz von \emph{Grafana} und
\emph{Ansible} erfüllt. Die funktionale Anforderung an eine native 1
Gigabit Ethernet Schnittstelle (\textbf{FA8}, siehe \autoref{table:FA8})
wird durch den Einsatz des \emph{Odroid}, als Einplatinenrechner,
eingehalten. Der Einsatz des Einplatinenrechners deckt sich ebenfalls
mit der nicht\hyp{}funktionalen Anforderung \textbf{NFA2} (Niedrige
Beschaffungskosten, siehe \autoref{table:NFA2}). Außerdem deckt der
Einsatz des Konfigurationsmanagement\hyp{}Werkzeugs \emph{Ansible} die
nicht\hyp{}funktionale \textbf{NFA4} (Horizontale Skalierbarkeit,
\autoref{table:NFA4}) ab. Übrig bleibt die funktionale Anforderung
\textbf{FA7} (Bandbreitenmessung, siehe \autoref{table:FA7}) und die
nicht\hyp{}funktionalen Anforderungen \textbf{NFA3} (Sicherheit, siehe
\autoref{table:NFA3}) und \textbf{NFA1} (Wahl der Programmiersprache,
siehe \autoref{table:NFA1}). Eine Bandbreitenmessung ist bedingt durch
den Einsatz des \emph{Prometheus Node Exporters} möglich. Der
\emph{Prometheus Node Exporter} ist durch den Einsatz der Metrik
\emph{node\_network\_transmit\_bytes} in der Lage die übermittelte Anzahl
der Bytes via Netzwerk zu messen\cite{HEISENODE}. Eine volle Auslastung
der Netzwerkkarte kann durch den Einsatz von Werkzeugen wie \emph{Iperf}
erreicht werden. Bei \emph{Iperf} handelt es sich um quelloffene
Software zum Testen von Rechnernetzen\cite{IPERF}. Das einzige Problem
stellt eine Automatisierung solcher Tests dar. Möglich wäre auch hier
eine Automatisierung via \emph{Ansible} oder via \emph{Systemd}
beziehungsweise \emph{Cronjobs} um die Ausführung auf dem Host
automatisiert zu einem bestimmten Zeitpunkt zu starten. Um das
Rechnernetz nicht permanent zu belasten habe ich mich für eine
Bandbreitenmessung via \emph{Iperf} und \emph{OpenSSH} entschieden. Der
Administrator loggt sich via \gls{ssh} auf dem Zielsystem ein und führt
\emph{Iperf} manuell aus. Die Ergebnisse einer solchen Messung wären via
\emph{Node Exporter} und \emph{Grafana} sichtbar. Zur Gewährleistung der
Sicherheit des Systems (nicht\hyp{}funktionale Anforderung
\textbf{NFA3}) findet die Installation und Konfiguration der Hosts nur
via \gls{ssh} statt. Die Messdaten werden via \gls{http}
übermittelt. Dieser Schritt ist dem Ursprung von \emph{Prometheus}
geschuldet. \emph{Prometheus} war eigentlich nie gedacht zur Überwachung
von Hosts über das Internet. Der Einsatz von \emph{Prometheus}
beschränkte sich auf den Einsatz in Container\hyp{}Umgebungen wie
\emph{Docker} oder Container\hyp{}Clustern wie \emph{Kubernetes}. Erst
nach und nach wurde \emph{Prometheus} zur Überwachung von virtuellen
Maschinen oder echter Hardware eingesetzt. Die Lösung des Problems ist
der Einsatz eines Webservers, welcher \gls{https} beherrscht. Dadurch
ist es möglich die Messdaten über den Webserver via \gls{https}
anzubieten. Dennoch wären die Messdaten auch von \gls{ip}\hyp{}Adressen
aus sichtbar, welche eigentlich keinen Zugriff auf die Messdaten haben
sollten. Es handelt sich zwar bei den Messdaten um keine kritischen
Informationen, allerdings geben die Messdaten einen Überblick über das
System (laufende Prozesse, Versionen von Software, Speicherplatz,
CPU\hyp{}Verbrauch, etc). Eine Zugangsbeschränkung ist durch den Einsatz
von einer \emph{Firewall} auf den Systemen möglich. Diese
\emph{Firewall} würde nur die für die Messung nötigen Verbindungen
erlauben. Implementierungen einer solchen \emph{Firewall} sind
beispielsweise \emph{Iptables}, \emph{Firewalld}, oder \emph{UFW}. Die
Konfiguration und Installation der \emph{Firewall} sind via
\emph{Ansible} unter geringem Aufwand möglich. Die
nicht\hyp{}funktionale Anforderung \textbf{NFA1} (Wahl der
Programmiersprache) ist ebenfalls nur bedingt erfüllt. Das
Konfigurationsmanagement ist zwar in der Sprache \emph{Python} verfasst
und mit \emph{Python} erweiterbar, allerdings sind alle anderen
Bestandteile in der Sprache \emph{Golang} geschrieben. Es ist jedoch
möglich Messdaten via eigenen \emph{Python}\hyp{}Modulen an
\emph{Prometheus} unter Verwendung des \emph{Pushgateways}
weiterzuleiten. Außerdem ist es möglich einen kleinen Webserver via
\emph{Python} zu schreiben, welcher die Daten \emph{Prometheus}
anbietet. Das \emph{Prometheus}\hyp{}Projekt bietet mit dem
\emph{Prometheus Python Client} eine Bibliothek zur Verwendung von
\emph{Prometheus} Komponenten mit \emph{Python}
an\cite{PROMETHEUSPYTHON}.
\chapter{Fazit und Ausblick}
Im Zentrum dieser Bachelorarbeit stand die Forschungsfrage: \emph{Wie
lassen sich Rechnernetze dezentral und skalierbar überwachen?} Diese
Bachelorarbeit stellt einen Versuch dar, diese Frage zu beantworten. Als
Forschungsmethode wurde das \emph{Prototyping} gewählt, da durch das
\emph{Prototyping} es möglich war während der Entwicklung Anreize des
Rechenzentrums der Technischen Universität Clausthal zu berücksichtigen.
Außerdem ist durch das \emph{Prototyping} der direkte Einsatz in der
Praxis ersichtlich. Der im Rahmen dieser Bachelorarbeit erschaffene
Prototyp, wuchs zu einem komplexen aber überschaubaren System, welches
in der Lage fast alle an das Projekt gestellten Anforderungen zu
erfüllen. Dabei wurden modernste Technologien und Standards verwendet,
welche teilweise ihren Ursprung in der Cloud\hyp{}Technologie
oder Container\hyp{}Technologie haben. Diese Technologien sind
von immenser Bedeutung für das heutige Internet und den damit
verbundenen Firmen. Eine Weiterentwicklung dieses Prototypen durch das
Rechenzentrum der Technischen Universität Clausthal ist nicht
auszuschließen. Das System stellt den Grundstein für ein modernes und
skalierbares Monitoring\hyp{}System dar. Die Skalierbarkeit wird durch den
Einsatz von \emph{Ansible} sichergestellt. Durch den Einsatz von
\emph{Prometheus} und dessen \emph{Blackbox Exportern} ist es möglich
verteilt Messpunkte zu schaffen, welche vom jeweiligen Standort aus,
Messdaten dezentral sammeln. Diese Messdaten werden von
\emph{Prometheus} abgeholt und von \emph{Grafana} für den Nutzer
grafisch aufbereitet. Mögliche Verbesserungen wurden in der
abschließenden Anforderungsanalyse dargelegt. Denkbare weitere
Verbesserungen für die Zukunft wären die Nutzung von \emph{Prometheus}
zur Überwachung anderer Dienste, der Einsatz von
Container\hyp{}Technologien für noch schnelleres Deployment und
eine Auslegung des Systems auf Hochverfügbarkeit. Eine solche
Hochverfügbarkeit könnte möglicherweise durch den Einsatz von
\emph{Kubernetes} und \emph{Openstack} erreicht werden. In dem durch den
Einsatz von \emph{Openstack} eine Rechenzentrum eigene
Cloud\hyp{}Infrastruktur aufgebaut wird, welche durch den Einsatz
von virtuellen Maschinen einen \emph{Kubernetes} basierten
Container\hyp{}Cluster etabliert. Abschließend möchte ich als Autor ein
kleines persönliches Fazit vermitteln. Diese Bachelorarbeit vereint
diverse Module meines Studiums. Ich habe mir nicht nur durch diese
Arbeit eine neue Programmiersprache (\emph{Golang}) angeeignet, sondern
auch meine Kenntnisse des \emph{Software\hyp{}Engineering}, der
Betriebssysteme und der Rechnernetze vertieft. Ebenfalls wurde ein
tiefer Einblick in aktuelle Technologien wie \emph{Ansible},
\emph{Prometheus} und \emph{Grafana} gewonnen. Ich bin für diese
Erkenntnisse äußerst dankbar und hoffe, dass die Technische Universität
Clausthal ihre Forschungen in Themengebiete wie \emph{Cloud Computing}
vertiefen wird.

\nocite{*}
\printbibliography{}
\lstlistoflistings{}
\listoftables{}
\listoffigures
\printglossary{}
\end{document}
